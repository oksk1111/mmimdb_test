멀티모달 융합 기반 영화 장르
예측 모델 개발 및 성능 평가
: MM-IMDb 데이터셋을 활용한 연구

Multi-modal Movie Genre Prediction Model Development
and Performance Evaluation using the MM-IMDb Dataset

2025년 12 월
서강대학교 AI SW 대학원
데이터사이언스 인공지능 전공
민 선 기
멀티모달 융합 기반 영화 장르
예측 모델 개발 및 성능 평가
: MM-IMDb 데이터셋을 활용한 연구

Multi-modal Movie Genre Prediction Model Development
and Performance Evaluation using the MM-IMDb Dataset
지도교수 박 운 상

이 논문을 공학 석사 학위논문으로 제출함
2025 년 11 월 28 일
서강대학교 AI SW 대학원
데이터사이언스 인공지능 전공
민 선 기
<차 례>

제1장. 서론	1
        1.1. 연구 배경 및 동기	2
        1.2. 연구 목표	3
        1.3. 논문 	4
제2장. 관련 연구 및 배경	5
        2.1. 선행연구	5
        2.2. 멀티모달 영화 장르 예측과 MM-IMDb 데이터셋	6
        2.3. 멀티모달 융합 전략	6
        2.4. ViT (Vision Transformer)와 ResNet50	8
        2.5. BERT 및 RoBERTa	8
제3장. 연구 방법론	10
        3.1. 제안하는 방법	10
            3.1.1. 연구 개요	10
            3.1.2. 데이터 개요 및 설명	10
            3.1.3. 데이터 특성	12
            3.1.4. 데이터 전처리	15
            3.1.5. 모델 적용 방법	15
        3.2. 모델 학습 설정	16
        3.3. 모델 성능 평가 지표	17
        3.4. 연구 방법론 정리	19
제4장. 분석 결과	21
        4.1. 단일모달 모델의 성능 비교	21
        4.2. 멀티모달 융합 모델의 성능 분석	22
        4.3. 성능 향상 분석	24
        4.4. 결과 요약 및 논의	26
제5장. 결론 및 시사점	27
        5.1. 연구 요약 및 주요 결과	27
        5.2. 시사점	28
        5.3. 연구 한계 및 향후 연구 방향	29
        5.4. 종합 결론	29

<표 차례>

<표 1> MM-IMDb 데이터셋 분할 현황	11
<표 2> MM-IMDb 데이터셋 분할 장르별 분포	12
<표 2> MM-IMDb 데이터셋의 목표변수와 설명변수	13
<표 3> MM‑IMDb 데이터셋의 장르 구성	14
<표 4> 실제 데이터 샘플 예시 (MM‑IMDb Test Dataset 일부)	14
<표 5> 단일모달 모델별 성능 비교	21
<표 6> 멀티모달 융합 모델별 성능 비교	23
<표 7> 단일모달 대비 멀티모달 성능 향상 요약	24


<그림 차례>

<그림 1> 단일모달 대비 멀티모달 모델의 성능 비교	25
<그림 2> 모델 유형별 성능 비교 차트	25


Abstract

Multilabel movie genre classification is a complex prediction task that requires comprehensive interpretation of diverse information such as text and images. To accurately classify the genre of movie content, an architecture capable of integrating semantic meaning across different modalities—such as plot summaries and poster images—is essential. Therefore, developing multimodal classification models that can simultaneously analyze both text and images is crucial. Recently, active research has been conducted to improve genre prediction accuracy using deep learning-based multimodal fusion strategies.
This paper proposes a multimodal classification model for predicting movie genres using the MM-IMDb dataset and compares the performance of unimodal models with various fusion strategies. The experiments were conducted on text-based (BERT), image-based (ResNet50, ViT), and multimodal fusion-based (Early Fusion, Late Fusion, GMU, Cross-Attention Fusion) models, evaluated using Accuracy and F1 score (Macro, Micro, Weighted, Samples).
The experimental results show that the text-based BERT model achieved the highest performance in the unimodal setting, and all multimodal fusion models outperformed their unimodal counterparts. In particular, the Cross-Attention Fusion model achieved the best performance by structurally aligning semantic information between text and images. This study empirically demonstrates that multimodal fusion strategies can overcome the limitations of unimodal approaches and that structural design plays a decisive role in enhancing predictive performance.  
초록

멀티레이블 영화 장르 분류는 텍스트와 이미지 등 다양한 정보를 종합적으로 해석해야 하는 복합 예측 문제이다. 영화 콘텐츠의 장르를 정확히 분류하기 위해서는 줄거리와 포스터 이미지처럼 서로 다른 모달리티 간의 의미를 통합하는 구조가 필요하다. 따라서 텍스트와 이미지를 동시에 분석할 수 있는 멀티모달 분류 모델의 개발이 중요하다. 최근에는 딥러닝 기반의 멀티모달 융합 전략을 통해 장르 예측의 정확도를 높이려는 연구가 활발히 진행되고 있다.
본 논문에서는 MM‑IMDb 데이터셋을 활용하여 영화 장르를 예측하는 멀티모달 분류 모델을 제안하고, 단일모달 모델과 다양한 융합 전략의 성능을 비교하였다. 실험은 텍스트 기반(BERT), 이미지 기반(ResNet50, ViT), 그리고 멀티모달 융합 기반(Early Fusion, Late Fusion, GMU, Cross-Attention Fusion) 모델을 대상으로 수행되었으며, Accuracy와 F1 Score(Macro, Micro, Weighted, Samples)를 기준으로 평가하였다.
실험 결과, 텍스트 기반의 BERT 모델이 단일모달 환경에서 가장 높은 성능을 보였고, 멀티모달 융합 모델은 모든 경우에서 단일모달 대비 성능이 향상되었다. 특히 Cross-Attention Fusion 모델은 텍스트와 이미지 간의 의미적 정렬을 구조적으로 달성함으로써 가장 우수한 성능을 기록하였다. 본 연구는 멀티모달 융합 전략이 단일모달 접근의 한계를 극복하고, 구조적 설계 방식이 예측 성능 향상에 결정적 역할을 한다는 점을 실증적으로 보여준다.

    제1장. 서론

최근 인공지능의 발전은 산업 전반의 변화를 이끌고 있으며, 특히 이미지와 텍스트 등 이질적인 데이터를 동시에 처리하는 멀티모달 학습(multimodal learning) 은 연구의 핵심 주제로 부상하였다. 현실 세계의 정보는 단일한 형태로 존재하지 않고, 언어·시각·청각 신호가 복합적으로 얽혀 제공된다. 예컨대 영화 콘텐츠를 소비하는 과정에서도 사용자는 포스터의 시각적 심상, 줄거리 텍스트, 예고편의 분위기 등을 종합적으로 고려한다. 따라서 서로 다른 모달리티 간 결합과 상호작용을 학습할 수 있는 인공지능 모델을 개발하는 것은 학문적으로나 실용적으로 모두 중요한 과제가 된다.
기존의 영화 장르 분류 연구는 주로 텍스트 기반의 줄거리 또는 이미지 기반의 포스터 단독 접근에 머물러 왔다[1]. 그러나 영화는 시각적 스타일과 서사적 구조가 결합된 매체라는 점에서, 단일 모달 접근은 필연적으로 한계를 가진다. 실제로 멀티모달 연구들은 서로 다른 모달리티 간 의미 정렬을 통해 성능이 향상됨을 보여주고 있다[2][3]. 이러한 맥락에서 다양한 멀티모달 융합 전략이 제안되어 왔다. 가장 단순한 Early Fusion과 Late Fusion, 그리고 모달리티별 중요도를 학습하는 Attention Fusion이 대표적이다. 또한 게이트 기반 융합 구조인 GMU(Gated Multimodal Unit)는 모달리티별 기여도를 동적으로 조절하여 Early/Late Fusion의 한계를 보완하는 방식으로 주목받았다[4]. 최근에는 Transformer 기반의 Cross-Attention 구조가 등장하여 텍스트와 이미지 간 교차 의미 학습을 가능케 하면서 멀티모달 연구의 새로운 방향을 제시하고 있다[5].
이에 본 연구는 MM‑IMDb 데이터셋을 활용하여 영화의 포스터 이미지와 줄거리 텍스트 두 가지 모달리티를 통합적으로 고려하는 멀티모달 학습 모델을 설계하였다. 특히 이미지 단일모달 모델(ResNet50, ViT), 텍스트 단일모달 모델(BERT, RoBERTa)을 기준점으로 설정한 뒤, Early Fusion, Late Fusion, Attention Fusion, GMU, Cross-Attention Fusion 등 다양한 멀티모달 융합 방법론을 실제 실험 코드 수준에서 구현하여 그 성능을 비교·분석하였다.

        1.1. 연구 배경 및 동기
멀티모달 학습 연구가 주목받는 이유는 크게 두 가지 측면에서 설명된다.
첫째, 데이터 환경의 변화이다. 대규모 온라인 플랫폼과 미디어 산업은 텍스트, 이미지, 영상, 음성 데이터가 동시에 수집·생산되는 구조를 갖추고 있다. 예를 들어, 온라인 영화 플랫폼에서는 줄거리 데이터와 비주얼 데이터가 함께 제공되며, 이를 통합적으로 분석할 수 있는 연구 수요가 증가한다.
 둘째, 기술 혁신의 진전이다. 과거의 단순 융합 방식인 Early Fusion과 Late Fusion은 구현이 용이했으나 모달 간 의미적 상호작용을 정교하게 반영하지 못하였다. 이를 보완하기 위해 게이트 기반 융합 구조인 GMU(Gated Multimodal Unit)가 제안되어, 모달리티별 기여도를 동적으로 조절하는 방식으로 성능 향상을 보여주었다. 이어서 Transformer 아키텍처와 Attention 메커니즘의 발전은 Cross-Attention 구조를 가능케 하여, 텍스트와 이미지 간 교차 의미 학습을 통해 더욱 심층적인 융합을 실현하였다.
연구 주제로서 영화 장르 예측이 선택된 것은, 영화라는 콘텐츠가 본질적으로 멀티레이블 구조를 띠고 있기 때문이다. 하나의 영화가 동시에'드라마'이자 '코미디'일 수 있다는 복합적 성격은, 두 모달리티인 포스터와 줄거리가 어떻게 협력적으로 작동하는지를 관찰하는 데 적합하다. 따라서 본 연구는 영화 장르 예측을 통해 멀티모달 융합 전략이 가지는 실질적 가능성과 한계를 확인하는 학문적 실험을 수행하고자 한다.

        1.2. 연구 목표
본 연구의 궁극적 목표는 영화 장르 예측을 넘어, 이를 매개로 멀티모달 융합의 가능성을 실증적으로 검토하는 데 있다. 이에 따라 구체적으로 다음과 같은 세부 목표를 설정한다.
        1. 모달리티 효과 비교: 텍스트 단독, 이미지 단독 모델 대비, 이 둘을 융합했을 때 어떤 성능적 차이가 나타나는지 평가한다.
        2. 융합 전략 검증: Early Fusion, Late Fusion, Attention Fusion, GMU 등 기존 방식과 Cross-Attention 기반 융합 구조를 동일 조건에서 비교하여 장단점을 분석한다.
        3. 실험적 탐색으로서의 기여: 본 연구의 성과는 제한적인 범위에서 얻어진 것이며, 모든 결과가 일반화되기엔 부족할 수 있다. 그러나 멀티모달 연구가 특정 응용 분야에서 어떻게 성과를 검증하는 탐색적 연구로서 가치가 있다.
이러한 배경에 따라 본 논문은 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), F1 Score 등 정량적 지표를 활용하여 멀티모달 융합의 성과를 검증하되, 그 결과가 멀티모달 연구 방법론의 가능성을 평가하는 근거로 기능하는 것을 목표로 삼는다.

        1.3. 논문 구성
본 논문은 총 다섯 개 장으로 구성된다.
제1장 서론에서는 연구 동기와 배경, 연구 목표를 설명하고, 영화 장르 분류를 멀티모달 평가 과제로 설정한 이유를 제시한다.
제2장 관련 연구 및 배경에서는 멀티모달 학습 관련 이론과 주요 기술(ResNet50, ViT, BERT, RoBERTa, 멀티모달 융합 전략)을 다룬다.
제3장 연구 방법론에서는 데이터셋 특성, 전처리 과정, 모달리티별 인코더 구조, 다양한 융합 전략 및 학습 환경을 설명한다.
제4장 실험 및 결과에서는 다양한 융합 구조와 단일 모달 모델을 비교하고, 영화 장르 분류라는 과제를 통해 멀티모달 접근의 성과와 한계를 논의한다.
제5장 결론에서는 연구의 요지를 요약하고, 멀티모달 융합 연구의 향후 방향을 제안한다.

    제2장. 관련 연구 및 배경

    2.1. 선행연구
멀티모달 학습은 서로 다른 유형의 데이터를 결합하여 더 풍부한 의미를 학습하는 인공지능의 핵심 연구 영역으로 자리 잡아왔다. 기존의 연구들은 텍스트와 이미지를 각각 독립적으로 처리하거나 단순히 결합하는 수준에 머물렀으나, 실제 사회적 맥락에서는 언어와 시각 정보가 동시에 존재한다는 점에서 두 모달리티를 결합한 학습이 점차 필요해졌다.
대표적인 사례로 Radford 등(2021)이 제안한 CLIP은 대규모 이미지-텍스트 쌍 데이터를 학습해 서로 다른 모달리티를 의미 공간에서 정렬하는 데 성공하였다[2]. CLIP은 새로운 이미지와 설명 텍스트에 대해 사전 학습을 통해 획득한 지식을 활용하여 추가 학습 없이도 제로샷(Zero-shot) 분류를 수행할 수 있다는 점에서 큰 주목을 받았다. 이어서 Jia 등(2021)은 ALIGN 모델을 제시하여 웹에서 수집한 노이즈가 많은 대규모 이미지-텍스트 데이터를 통해 보다 일반화된 멀티모달 표현 학습을 가능케 하였다[3]. 또한 Li 등(2022)은 BLIP 모델을 통해 이미지 설명 생성(Image Captioning)과 시각적 질문 응답(VQA) 등 다중 과제에서 활용할 수 있는 통합형 사전학습 구조를 제안하였고[6], Liu 등(2023)의 LLaVA는 대규모 언어 모델(LLM)에 시각적 이해 능력을 결합하여 멀티모달 대화형 AI까지 확장하는 성과를 보여주었다[7].
이와 같은 최근 연구들은 단순히 텍스트와 이미지를 합치는 수준을 넘어서, 두 모달 간에 정교한 상호작용을 학습하는 방향으로 진화하고 있다. 특히 Kiela 등(2019)의 연구에서 논의된 바와 같이, Transformer 기반의 Cross-Attention 구조는 텍스트 내 단어 토큰과 이미지 내 특정 영역이 직접 연결되어 의미를 교차적으로 학습할 수 있게 한다[4]. 이러한 흐름은 광고 분석, 의료 영상과 보고서 결합, 소셜미디어 멀티모달 분석 등 다양한 실질적 응용으로 확장되며 멀티모달 연구의 핵심적 발전을 이끌고 있다.

    2.2. 멀티모달 영화 장르 예측과 MM-IMDb 데이터셋
MM-IMDb는 영화 장르 예측을 위한 대표적인 멀티모달 데이터셋으로, 약 25,000편 이상의 영화를 포함하고 있으며 각 영화에는 포스터 이미지, 줄거리 텍스트, 그리고 23개의 장르 멀티레이블이 제공된다. 이 데이터셋은 한 영화가 복수의 장르에 동시에 속할 수 있도록 구성되어 있으며, 이는 실제 영화 산업의 속성을 잘 반영한다. 기존 연구들은 CNN 및 ResNet 기반 모델을 활용하여 포스터 이미지를 인코딩하고, Word2Vec, GloVe, 그리고 최근에는 BERT와 같은 Transformer 기반 모델을 사용하여 줄거리 텍스트를 임베딩한 뒤 단순 결합하는 방식을 채택하는 경우가 많았다. 그러나 이러한 방식은 이미지와 텍스트 사이의 깊이 있는 상호작용을 충분히 반영하지 못하는 한계가 존재한다. 따라서 본 연구에서는 MM-IMDb 데이터셋을 바탕으로 이러한 약점을 극복하고자 하며, 특히 Cross-Attention 구조를 적용하여 기존 방식보다 더 풍부한 모달 간 연결 구조를 구현하고자 한다.

    2.3. 멀티모달 융합 전략
멀티모달 학습의 중심 과제는 서로 다른 모달리티로부터 추출된 특징들을 효과적으로 결합하는 것이다. 기존 연구에서 가장 단순한 접근은 Early Fusion 방식이었다[5]. Early Fusion은 텍스트와 이미지에서 얻은 특징 벡터를 단순히 이어 붙여 하나의 입력으로 분류기에 전달하는 방식이다. 구조가 단순하고 계산량이 적다는 장점이 있으나, 모달 간 복잡한 의미적 관계를 충분히 반영하지 못한다는 한계를 가진다.
Late Fusion은 각 모달리티를 독립적으로 학습시킨 후 최종 단계에서 예측 확률이나 결과를 병합하는 방식이다[4]. 장점은 특정 모달에서 잡음이나 결손이 발생하더라도 다른 모달의 학습 결과가 이를 보완할 수 있다는 점이다. 그러나 두 모달 간의 상관관계를 직접적으로 학습하지 않기 때문에 융합의 효과가 제한적이라는 문제가 있다.
이후 Attention Fusion이 제안되었다. Attention Fusion은 모달리티별 특징 가운데 중요한 요소에 가중치를 두어 학습의 효율성을 높인다. 예를 들어 텍스트가 감정 신호를 더 잘 드러내는 경우 텍스트에 높은 가중치가, 이미지가 더 중요한 경우 이미지에 높은 가중치가 부여된다. 그러나 이 방식 역시 모달 간의 구조적 상호작용을 깊이 있게 반영하지는 못한다는 한계가 있다. 이러한 한계를 보완하기 위해 GMU(Gated Multimodal Unit)가 제안되었다[1]. GMU는 게이트 메커니즘을 활용하여 모달리티별 기여도를 동적으로 조절한다. 즉, 텍스트와 이미지 특징을 입력받아 상황에 따라 어느 모달리티가 더 중요한지를 학습적으로 결정한다. 이를 통해 Early/Late Fusion보다 정교한 융합을 가능케 하며, 다양한 멀티모달 과제에서 성능 향상을 보여왔다.
최근에는 Transformer 구조를 확장한 Cross-Attention 방식이 널리 활용되고 있다[4][5]. Cross-Attention에서는 텍스트와 이미지가 각각 Query, Key, Value 역할을 수행하면서 서로 정보를 교환한다. 즉, 텍스트 토큰이 이미지의 특정 패치와 상호작용하거나, 이미지의 세부 영역이 텍스트의 특정 단어와 대응하여 의미를 공유하게 된다. 이런 방식은 단순한 가중치 조정이 아니라, 두 모달리티 사이에 심층적인 교차 의미 학습을 가능케 하므로 최근 멀티모달 분류, 검색, 생성 연구에서 가장 각광받는 융합 전략 중 하나로 자리 잡았다.
    2.4. ViT (Vision Transformer)와 ResNet50
이미지 처리 분야는 전통적으로 합성곱 신경망(CNN, Convolutional Neural Network)을 기반으로 발전해왔다. ResNet50은 CNN 아키텍처에서 가장 대표적인 모델 중 하나로, '잔차 연결(residual connection)'을 도입하여 매우 깊은 네트워크에서도 기울기 소실 문제를 해결한 구조이다[8]. 이 모델은 국소적인 이미지 특징을 안정적으로 추출할 수 있으며, 특정 객체, 질감, 패턴과 같은 부분적 단서 인식에 강점을 가진다. 예를 들어 영화 포스터에서 무기, 특정 의상 스타일, 로고 같은 요소는 지역적인 특징이기 때문에 ResNet50으로 잘 인식될 수 있다.
반면 최근 제안된 ViT는 Transformer 구조를 시각적 데이터에 적용한 모델이다[9]. ViT는 입력 이미지를 작은 패치 단위로 분할하여 각 패치를 임베딩 벡터로 변환하고, 이를 텍스트 처리와 유사하게 Self-Attention 구조에 입력한다. 이 과정에서 이미지 전체의 전역적 문맥을 효과적으로 학습할 수 있다. 이는 CNN이 주로 지역적인 패턴 인식에 강점을 가진 반면, ViT는 이미지 전체의 배치, 색감, 레이아웃과 같은 전역적 관계를 포착하는 데 강점을 갖는다. 따라서 두 모델은 상호 보완적인 관계에 있으며, 본 연구에서는 ViT와 ResNet50을 모두 활용하여 이미지 모듈의 성능을 비교하고 각 모델이 제공하는 특징적 차이를 분석한다.

    2.5. BERT 및 RoBERTa
자연어 처리(NLP) 분야에서는 Transformer 기반 언어 모델의 등장이 혁신적 변화를 이끌었다. 그중 BERT(Bidirectional Encoder Representations from Transformers)는 문맥을 양방향으로 학습하는 구조를 통해 단어의 의미를 정밀하게 추출할 수 있도록 고안되었다[10]. 예를 들어 “dark”라는 단어가 특정 문맥에서 '조도가 낮다'는 의미일 수도 있고, 다른 문맥에서 '음산하다'라는 정서를 드러낼 수도 있는데, BERT는 앞뒤 문맥 정보를 모두 고려하여 이런 의미 차이를 구체적으로 반영할 수 있다.
이후 BERT의 개선형 모델로 RoBERTa(Robustly Optimized BERT Pretraining Approach)가 제안되었다[11]. RoBERTa는 더 많은 학습 데이터와 더 긴 훈련 기간을 도입하고, 사전 학습 시 마스킹 전략을 개선하여 BERT보다 표현력이 강력한 모델로 발전하였다. 그 결과, 다양한 자연어 처리 과제에서 일관되게 BERT보다 높은 성능을 달성하였다. 이러한 특성으로 인해 BERT와 RoBERTa는 단순 분류를 넘어, 줄거리 텍스트와 같은 서사적 자료에서 등장인물, 사건, 정서적 분위기 등 장르를 암시하는 다양한 단서를 효과적으로 추출할 수 있는 핵심 모델로 활용되고 있다.
    제3장. 연구 방법론

    3.1. 제안하는 방법
            3.1.1. 연구 개요
본 연구는 멀티모달 융합 기반 영화 장르 예측 모델을 개발하고 그 성능을 검증하기 위해 수행되었다. 기존 연구들이 이미지 또는 텍스트와 같은 단일 모달리티 분석에 집중해 온 반면, 영화라는 매체는 본질적으로 시각적·언어적 신호가 복합적으로 작동하는 멀티모달 데이터에 기반한다. 따라서 본 연구에서는 이미지(포스터)와 텍스트(줄거리)라는 두 가지 주요 모달리티를 통합하여 영화 장르를 예측하는 시스템을 설계하였다.
제안 모델은 MM-IMDb 데이터셋을 기반으로 하며, 각 모달리티의 개별 특징을 추출한 뒤 다양한 융합 방식을 통해 장르 분류를 수행한다. 비교 모델로는 단일모달 모델(ResNet50, ViT, BERT, RoBERTa)과 멀티모달 융합 모델(Early Fusion, Late Fusion, Attention Fusion, GMU, Cross-Attention Fusion)을 포함한다.
본 장에서는 데이터의 특성과 전처리 과정, 각 모달리티별 특징 추출 방법, 융합 전략, 모델 학습 및 평가 방법을 차례로 설명한다.

            3.1.2. 데이터 개요 및 설명
본 연구에서 활용한 데이터셋은 MM-IMDb (Multi-modal IMDb Dataset) 로, 약 25,959편의 영화를 포함한다. 각 샘플은 영화 포스터 이미지, 줄거리 텍스트, 그리고 다중 레이블 장르 정보로 구성되어 있으며, 멀티모달 학습 연구에 널리 사용되는 대표적 벤치마크 데이터셋이다.
데이터 분할은 제공자가 제시하는 기본 분할을 그대로 사용하였다. 이 비율은 Arevalo 등(2017)과의 비교 실험을 위해 해당 논문에서 사용된 분할 비율과 동일하게 학습 데이터셋 약 60%(15,552편), 검증 데이터셋 약 10%(2,608편), 테스트 데이터셋 약 30%(7,799편)과 분류 클래스인 장르를 23개를 사용하였다. 또한 각 장르별 분포가 균형을 이루도록 전처리를 수행하였다.

<표 1> MM-IMDb 데이터셋 분할 현황
구분
샘플 수
비율(%)
설명
학습 데이터
15,552
약 59.9%
모델 학습용 데이터
검증 데이터
2,608
약 10.0%
하이퍼파라미터 조정 및 과적합 검증
테스트 데이터
7,799
약 30.0%
최종 성능 평가에 사용
합계
25,959
100%
전체 데이터셋


<표 2> MM-IMDb 데이터셋 분할 장르별 분포
Genre
Train
Dev
Test
Genre
Train
Dev
Test
Drama
8343
1394
4230
Family
997
169
502
Comedy
5168
856
2568
Biography
810
127
406
Romance
3235
527
1602
War
800
136
399
Thriller
3109
538
1545
History
699
118
326
Crime
2306
387
1145
Music
626
110
309
Action
2107
365
1078
Animation
599
98
300
Horror
1635
267
801
Musical
499
86
256
Adventure
1624
288
798
Western
424
70
211
Mystery
1227
210
620
Sport
380
64
190
Documentary
1223
213
646
Short
281
48
142
Sci-Fi
1190
206
595
Film-Noir
202
34
102
Fantasy
1157
199
577





            3.1.3. 데이터 특성
본 연구에서 활용한 MM‑IMDb 데이터셋은 영화 포스터 이미지, 줄거리 텍스트, 그리고 멀티레이블 장르 변수를 포함한다. 목표변수는 영화가 속한 장르(Genres)로 정의되며, 실제 실험에서는 총 23개 장르를 사용하였다[1]. 따라서 본 연구의 결과는 기존 연구와 직접적으로 비교 가능한 조건에서 도출되었다. 설명변수는 이미지 모달리티와 텍스트 모달리티를 중심으로 구성된다. 이미지 모달리티는 각 영화의 포스터 이미지이며 224×224 크기로 전처리되어 CNN, ViT 등의 인코더 입력으로 사용된다. 텍스트 모달리티는 줄거리(plot)로, 원문 텍스트를 전처리한 후 BERT 및 RoBERTa 토크나이저로 변환하여 input_ids와 attention_mask 형태의 벡터 입력으로 활용한다. 이 외에 데이터셋에는 movie_id와 같은 식별자 정보가 포함되어 있으나, 이는 실험에서 직접적인 설명변수로 사용되지 않았다.
아래 <표 3>은 본 연구에서 실제 실험에 활용된 MM-IMDb 데이터셋의 목표변수와 설명변수 구성을 요약한 것이다.

<표 3> MM-IMDb 데이터셋의 목표변수와 설명변수
No
변수명
구분
설명
0
Genres (labels)
목표변수 (Target)
영화의 장르를 나타내는 멀티레이블 변수. 총 23개 클래스를 사용하며, 한 영화는 여러 장르에 동시에 속할 수 있음. 신경망 모델의 최종 출력 레이블로 활용됨.
1
Poster Image (image)
설명변수 (Image Feature)
영화 포스터 이미지. 3채널 RGB 형식으로 불러온 뒤 224×224 크기로 리사이즈 및 정규화하여 CNN/ViT 등 이미지 인코더 입력으로 사용됨.
2
Plot (plot)
설명변수 (Text Feature – Raw)
영화 줄거리 원문 텍스트. 등장인물과 사건, 분위기 등 장르 단서를 언어적 신호로 포함. 전처리 과정을 거쳐 토크나이저에 입력됨.
3
Text Encoded (text: {input_ids, attention_mask})
설명변수 (Text Feature – Encoded)
줄거리 텍스트를 BERT/RoBERTa 토크나이저로 처리하여 생성된 인덱스(input_ids)와 입력 마스크(attention_mask). Transformer 기반 언어모델의 입력으로 직접 활용됨.
–
movie_id
식별자 (Not Used)
영화의 IMDb 고유 식별 번호. 데이터 관리 용도로만 사용되며 설명변수로 활용되지 않음.

<표 4> MM‑IMDb 데이터셋의 장르 구성
No
장르
No
장르
No
장르
1
Action
10
Fantasy
19
Sci‑Fi
2
Adventure
11
Film‑Noir
20
Short
3
Animation
12
History
21
Sport
4
Biography
13
Horror
22
Talk‑Show
5
Comedy
14
Music
23
Thriller
6
Crime
15
Musical
24
War
7
Documentary
16
Mystery
25
Western
8
Drama
17
News
26
Reality‑TV
9
Family
18
Romance



<표 5> 실제 데이터 샘플 예시 (MM‑IMDb Test Dataset 일부)
Key
Data Type
예시 값
movie_id
str
"0078718"
image
torch.Tensor
torch.Size([3, 224, 224]) (224×224 RGB 포스터 이미지 텐서)
plot
str
"When a judge is charged with rape, Arthur Kirkland is forced to defend him ..."
text
dict
{'input_ids': tensor([101, 2043, 1037, ...]), 'attention_mask': tensor([1, 1, 1, ...])}
labels
torch.Tensor
shape = [26], 예: [0, 0, 1, ..., 0] (멀티-핫 인코딩 벡터)
            3.1.4. 데이터 전처리
본 연구에서 사용된 MM‑IMDb 데이터셋은 이미지, 텍스트, 멀티레이블 장르 변수를 포함하고 있다. 따라서 각 데이터 유형에 맞추어 전처리 과정을 수행하였다.
이미지 전처리는 다음과 같이 수행되었다. 포스터 이미지는 일정한 크기(224×224)로 리사이징(Resizing) 하였으며, 학습 데이터의 일반화 성능을 높이기 위해 무작위 자르기(Random Cropping), 좌우 반전(Horizontal Flipping), 색상 변환(Color Jittering) 등의 데이터 증강(Data Augmentation) 기법을 적용하였다. 또한, 각 채널 별 평균과 표준편차를 기준으로 한 정규화(Normalization) 를 수행하였다.
텍스트 전처리는 영화 줄거리 텍스트로부터 소문자 표준화와 특수문자 제거 후, 토큰화(Tokenization) 를 거쳐 고정된 길이의 시퀀스로 변환하였다. 여기에는 최대 토큰 길이 제한(512토큰), 패딩(Padding) 처리, 그리고 어텐션 마스크(Attention Mask) 생성을 포함한다. 토큰화된 텍스트는 이후 Transformer 계열 언어모델 입력 벡터로 활용하였다.
레이블데이터는 영화 장르를 멀티레이블 벡터(Multi-hot Encoding) 방식으로 변환하였다. 총 26차원의 장르 벡터에서 영화가 속한 장르는 값 1, 해당하지 않는 장르는 값 0으로 할당되었다.

            3.1.5. 모델 적용 방법
본 연구에서는 다양한 모달리티 기반 모델의 성능을 비교 분석하기 위해 단일 모달리티 모델과 멀티모달 융합 모델 모두를 적용하였다. 이미지 단일 모달리티에서는 ResNet50과 ViT를 활용하여 포스터 이미지만으로 장르 분류를 시도하였다. 마찬가지로, 텍스트 단일 모달리티에서는 BERT와 RoBERTa를 사용하여 영화 줄거리로부터 장르를 예측하는 방식으로 모델을 설계하였다.
멀티모달 융합 모델은 이미지와 텍스트 정보를 효과적으로 결합할 수 있는 5가지 전략을 채택하였다. 먼저 Early Fusion 방식으로는 이미지 벡터와 텍스트 벡터를 단순연결(concatenate)하여 예측에 활용하였다. Late Fusion 방식에서는 각 모달리티에서 획득한 분류 결과를 가중 평균하는 방식으로 최종 예측값을 도출하였다. Attention Fusion 방식은 이미지와 텍스트의 각각의 특성이 예측에 미치는 상대적 중요도를 학습하는 방식을 적용하였고, GMU 방식에서는 게이트 매커니즘을 활용하여 모달리티별 기여도를 동적으로 조절하는 방식을 사용하였다. Cross-Attention Fusion 방식에서는 Query-Key-Value 구조를 바탕으로 이미지와 텍스트 간의 상호 의미 교환을 통합하여 장르를 예측하는 과정을 설계하였다.

        3.2. 모델 학습 설정
본 연구에서 제안한 단일모달 및 멀티모달 모델들은 모두 동일한 학습 환경에서 비교 실험을 진행하였다. 모델 학습은 GPU 기반의 고성능 연산 환경에서 수행되었으며, 모든 실험은 동일한 하이퍼파라미터 조건을 유지하여 모델 구조 간 성능 비교의 일관성을 확보하였다.
최적화 기법으로는 AdamW(Adaptive Moment Estimation with decoupled Weight Decay) 알고리즘을 사용하였다. AdamW는 기존 Adam의 적응적 학습률 구조를 유지하면서 가중치 감쇠 항을 학습률 갱신 단계와 분리하여 계산한다. 이러한 구조는 규제가 적용되는 시점을 명확히 분리함으로써 일반화 성능을 향상시키고, 과도한 규제에 의한 과소적합 문제를 방지한다. AdamW는 각 매개변수의 1차 및 2차 모멘트 추정 값을 기반으로 개별 학습률을 조정함으로써 안정적 수렴을 유도한다. 본 연구에서는 가중치 감쇠 계수를 로 설정하여 규제 효과를 조절하였다[13].
학습률은 로 설정하였으며, 배치 크기는 16, 학습 반복(epoch)은 10으로 지정하였다. 손실 함수로는 이진 교차 엔트로피(Binary Cross-Entropy with Logits Loss)를 적용하였다. 이 손실 함수는 각 장르가 서로 독립적인 확률 분포를 가진다는 점을 반영하여, 다중 장르(multi-label) 분류 문제를 효과적으로 처리할 수 있다.
모델 학습 과정에서는 검증 데이터셋의 성능 변화를 모니터링 함으로써 과적합 여부를 점검하였다. 학습이 진행되는 동안 검증 손실이 더 이상 개선되지 않거나 오히려 증가하는 경우에 학습을 조기 종료하는 방식을 통해 일반화 성능을 확보하였다.

        3.3. 모델 성능 평가 지표
모델의 성능은 단일 지표에 의존하지 않고, 멀티레이블 데이터의 불균형을 고려한 여러 지표를 통해 다각적으로 평가하였다. 본 연구에서는 정확도, 정밀도, 재현율, 그리고 F1 Score를 중심으로 성능을 측정하였다.
정확도는 전체 샘플 중 모델이 올바르게 예측한 비율을 측정하는 가장 기본적인 성능 지표이다. 하지만 멀티레이블 데이터의 특성상, 긍정·부정 클래스 간 데이터 불균형이 존재하기 때문에 정확도만으로는 성능을 충분히 평가하기 어렵다. 따라서 정밀도와 재현율을 함께 고려하였다.






TP (True Positive): 실제 True 이고, 예측도 True.
TN (True Negative): 실제 False이고, 예측도 False
FP (False Positive): 실제 False이나, 예측은 True
FN (False Negative): 실제 True이나, 예측은 False

정밀도는 모델이 '긍정'으로 분류한 표본 중 실제로 긍정인 비율로 정의되고, 재현율은 실제 긍정 표본 중 모델이 긍정으로 올바르게 예측한 비율이다. 이 두 지표는 상호 보완적 관계에 있으므로, 두 값을 종합한 F1 Score를 중심 지표로 활용하였다. F1 Score는 다음과 같이 정의된다.



특히, 본 연구에서는 Arevalo 등(2017)과 동일하게 F1 Score의 다양한 평균 방식을 적용하였다.
    • Macro: 각 클래스별 성능 지표를 따로 계산한 후, 단순히 산술 평균을 내는 방식이다. 모든 클래스를 동등한 중요도로 간주하며, 소수 클래스의 성능도 동일하게 반영된다. 클래스 불균형이 심한 경우 유용하다.
    • Micro: 모든 클래스의 TP(True Positive), FP(False Positive), FN(False Negative) 값을 전체적으로 합산한 후, 이로부터 정밀도, 재현율, F1 Score를 계산한다. 결과적으로 각 샘플을 동등하게 취급하며, 데이터 불균형의 영향을 덜 받는다.
    • Weighted: 각 클래스의 F1 Score등을 계산한 후, 해당 클래스의 샘플 수에 따라 가중 평균을 구한다. 샘플이 많은 클래스일수록 전체 지표에 더 큰 영향을 미친다. 데이터 분포를 반영하고 싶을 때 적합하다.
    • Samples: 주로 다중 레이블 분류에서 사용되며, 각 샘플별로 F1 Score를 계산한 후 평균을 낸다. 샘플 단위의 성능을 평가하고 싶을 때 사용한다.
이와 같이 다양한 F1 Score 평균 방식을 함께 제시함으로써, 본 연구는 모델이 클래스 불균형 상황에서 얼마나 균형적으로 성능을 발휘하는지(Macro, Weighted), 전체 데이터 수준에서 얼마나 정확한지(Micro), 개별 샘플 단위에서 얼마나 일관된 성능을 보이는지(Samples)를 종합적으로 검증하였다.

        3.4. 연구 방법론 정리
본 연구의 전체적인 연구 절차는 데이터 수집과 전처리, 모델 설계, 학습, 그리고 성능 평가의 네 단계로 구성된다.
먼저, MM‑IMDb 데이터셋을 학습, 검증, 테스트용으로 분할한 뒤, 이미지와 텍스트 데이터를 각각 모델에 적합한 형태로 전처리하였다. 이미지는 리사이징과 정규화, 색상 변화 등 데이터 증강 기법을 거쳐 학습 안정성을 확보하였고, 텍스트 데이터는 토큰화 및 패딩 과정을 거쳐 Transformer 기반 언어모델 입력 벡터로 변환되었다.
이후, 이미지 모달리티인 ResNet50, ViT와 텍스트 모달리티인 BERT, RoBERTa를 각각 독립적으로 학습하여 기준 성능을 확보하였다. 이러한 단일모달 결과를 바탕으로, 이미지와 텍스트를 결합한 다섯 가지 멀티모달 융합 구조인 Early Fusion, Late Fusion, Attention Fusion, GMU, Cross-Attention Fusion을 설계하였다. 각 융합 구조는 모달리티 간 상호작용의 깊이에 따라 정보 결합 방식을 달리하며, 이 상호작용 방식의 차이가 성능에 어떤 영향을 주는지를 분석한다.
본 연구의 최종 목적은 멀티모달 융합 전략별 성능 변화를 정량적으로 검증함으로써, 단일모달 접근의 한계를 보완할 수 있는 통합 학습 구조의 타당성을 검증하고, Cross-Attention Fusion 구조가 멀티모달 학습에서 가지는 잠재적 효과를 실험적으로 규명하는 데 있다.

    제4장. 분석 결과

    4.1. 단일모달 모델의 성능 비교
이미지 또는 텍스트 한 가지 모달리티만을 입력으로 활용한 실험을 수행하였다. 이미지 기반 모델은 영화 포스터를 입력으로 하며, ResNet50과 ViT를 비교 대상으로 하였다. 텍스트 기반 모델은 줄거리(plot) 데이터를 입력으로 하며, BERT와 RoBERTa를 각각 fine‑tuning하여 적용하였다.
<표 6> 단일모달 모델별 성능 비교
Model
Accuracy
F1-Macro
F1-Micro
F1-Weighted
F1-Samples
ResNet50
0.9042
0.1107
0.3798
0.2814
0.3773
ViT
0.9082
0.1759
0.4158
0.3466
0.3996
BERT
0.9215
0.3163
0.5419
0.4975
0.5375
RoBERTa
0.8963
0.0302
0.2988
0.1519
0.3135

이미지 기반 모델 중에서는 ViT가 ResNet50에 비해 전반적으로 우수한 성능을 보였다. 이는 두 모델의 구조적 설계 목적에서 기인한다. ResNet50은 합성곱 연산을 기반으로 하는 CNN 계열 네트워크로 포스터 내의 색상, 질감, 객체 형태 등 국소적인 시각적 특징을 중심으로 학습한다. 반면 ViT는 입력 이미지를 일정한 크기의 패치 단위로 분할한 후 각 패치를 시퀀스 형태로 임베딩하여 Self‑Attention 메커니즘을 통해 전역적인 관계를 학습한다. 이러한 차이로 인해 ViT는 장르별로 반복되는 색채 배합, 인물의 배치, 조명 대비와 같은 전반적 시각 구성과 분위기 정보를 더 효과적으로 포착할 수 있었다.
텍스트 모델 중에서는 BERT가 우수한 성능을 보였으며 이미지 기반 모델에 비해 모든 지표에서 30~185% 향상된 성능을 보였다. 특히 F1-Macro에서는 ResNet50 대비 185.7% 향상, ViT 대비 79.8% 향상으로 가장 큰 차이를 보였다. BERT의 양방향 문맥 학습 구조는 긴 문장 내 단어의 의미 관계를 정교하게 포착할 수 있도록 설계되어 있으며, 영화 줄거리처럼 인물과 사건이 계층적으로 얽히는 데이터에 효과적으로 대응한다.
 반면 RoBERTa는 기대 대비 낮은 성능을 보였다. RoBERTa는 웹 뉴스 기반의 단문 도메인 또는 일상 언어 데이터로 사전학습된 모델이기 때문에, 영화 줄거리처럼 서사가 길고 인물의 감정선이 반복되는 텍스트에서는 문장 간 의미 흐름의 일관성을 유지하기 어렵다. 이로 인해 모델의 Self‑attention 이 핵심 정보에 집중하지 못하고 전체 문맥에 분산되는 것으로 추정된다.  또한 RoBERTa가 사용하는 BPE (Byte‑Pair Encoding) 기반 SentencePiece 토크나이저는 긴 서사 구문을 세밀하게 단편화하여 희귀어 처리는 우수하지만, 연속적 감정 흐름을 표현하는 문맥에서는 필연적으로 문맥 연속성의 손실 을 야기할 수 있다. 이는 긴 서사 텍스트에서 토큰 단편화가 문맥 연속성을 감소시킬 수 있음을 시사한다. 이러한 조건들이 복합적으로 작용한 결과, RoBERTa의 예측 성능이 BERT보다 낮게 나타난 것으로 해석된다.
종합하면, 텍스트 단일모달 모델의 성능이 이미지 기반 모델보다 전반적으로 우세하였다. 이는 영화 장르 구분이 시각적 양식 보다는 줄거리 전개의 언어적, 서사적 구성에 더 의존함을 실증적으로 보여준다.

    4.2. 멀티모달 융합 모델의 성능 분석
멀티모달 융합 모델은 이미지와 텍스트 정보를 결합하여 영화 장르를 예측하는 방식으로, 본 연구에서는 Early Fusion, Late Fusion, Attention Fusion, GMU(Gated Multimodal Unit), Cross-Attention Fusion의 다섯 가지 전략을 비교하였다.
<표 7> 멀티모달 융합 모델별 성능 비교
Model
Accuracy
F1-Macro
F1-Micro
F1-Weighted
F1-Samples
Early Fusion
0.9240
0.3323
0.5714
0.5223
0.5706
Late Fusion
0.9217
0.2962
0.5377
0.4824
0.5309
Attention Fusion
0.9173
0.2423
0.4941
0.4279
0.4958
GMU
0.9278
0.3947
0.6041
0.5665
0.6060
Cross-Attention
0.9290
0.4696
0.6207
0.5987
0.6220

분석 결과, 모든 멀티모달 융합 모델이 단일모달에 비해 향상된 성능을 보였다.
GMU는 기존의 Early Fusion, Late Fusion, Attention Fusion보다 모든 지표에서 우수한 성능을 보였다. 특히 F1-Macro 값이 0.3947, F1-Samples 값이 0.6060으로, 단순 병합 방식보다 정교한 게이트 기반 융합이 효과적임을 실험적으로 입증하였다. 이는 GMU가 모달리티별 기여도를 동적으로 조절함으로써, 멀티레이블 분류에서 더 균형 잡힌 예측을 가능하게 한다는 점을 보여준다.
Cross-Attention Fusion은 GMU보다도 높은 성능을 기록하였다. F1-Macro 값이 0.4696, F1-Samples 값이 0.6220, Accuracy 값이 0.9290으로 모든 지표에서 최고 성능을 달성하였으며, 이는 텍스트와 이미지 간의 교차 의미 학습이 게이트 기반 융합보다 더 강력한 표현력을 제공함을 시사한다.
반면 Attention Fusion은 F1-Macro 값이 0.2423로 가장 낮은 성능을 보였고, Late Fusion은 F1-Macro 값이 0.2962에 그쳤다. Early Fusion은 단순 연결 방식임에도 불구하고 안정적인 성능을 보였으나, GMU 및 Cross-Attention Fusion에는 미치지 못했다.

    4.3. 성능 향상 분석
모든 주요 지표에서 멀티모달 접근의 의미 있는 향상이 확인되었다. 특히 F1-Macro 기준으로는 119.3% 향상되어, 멀티레이블 분류에서 소수 클래스까지 균형 있게 예측할 수 있는 능력이 크게 개선되었음을 의미한다.
F1-Micro, F1-Weighted, F1-Samples에서도 각각 38.3%, 62.7%, 38.8%의 향상률을 기록하며, 멀티모달 융합이 전체 예측 품질과 샘플 단위의 일관성 모두에 긍정적인 영향을 미친 것으로 나타났다.
이러한 결과는 멀티모달 접근이 단일모달보다 더 풍부한 표현력과 예측 성능을 제공하며, 특히 GMU와 Cross-Attention Fusion 같은 구조는 멀티레이블 영화 장르 예측에서 효과적인 전략임을 실증적으로 입증한다.

<표 8> 단일모달 대비 멀티모달 성능 향상 요약
모델 유형
Accuracy
F1-Macro
F1-Micro
F1-Weighted
F1-Samples
단일모달
0.9076
0.1583
0.4091
0.3194
0.4069
멀티모달
0.9239
0.3472
0.5656
0.5192
0.5649
향상률 (%)
+1.80%
+119.3%
+38.3%
+62.7%
+38.8%


<그림 1> 단일모달 대비 멀티모달 모델의 성능 비교


<그림 2> 모델 유형별 성능 비교 차트
    4.4. 결과 요약 및 논의
본 장의 실험 결과를 종합하면 다음과 같다.
첫째, 텍스트 기반의 BERT 모델이 단일모달 환경에서 가장 높은 예측 정확도를 보였는데, 이는 영화 줄거리 텍스트가 장르 구분의 핵심 단서임을 보여준다.
둘째, 멀티모달 융합 모델은 모든 경우에서 단일모달 대비 성능이 향상되었으며, 특히 Cross‑Attention Fusion 모델이 모든 지표에서 최고 성능을 기록하였다. 이 모델은 텍스트와 이미지 간 정보를 상호 보완적으로 결합하도록 설계되어, 두 모달리티 간의 의미적 정렬을 구조적으로 달성하였다.
셋째, Cross‑Attention Fusion 구조는 기존의 게이트 기반 융합 방식(GMU)보다도 더 높은 예측 성능을 보였으며, 단순 융합 방식인 Early Fusion에 비해 모달 간 상대적 정보를 깊이 있게 통합함으로써 예측 안정성과 일반화 능력을 동시에 확보하였다.
이상의 결과는 제안된 멀티모달 융합 모델이 영화 장르 예측 문제에서 단일모달 접근의 한계를 극복하고, 모델 간 정보교환 구조의 적절한 설계가 예측 성능 향상에 결정적 역할을 한다는 점을 실증적으로 보여준다. 또한 이러한 분석은 이후 제5장에서 다룰 연구의 한계 및 향후 발전 방향에 대한 근거를 제공한다.

    제5장. 결론 및 시사점

    5.1. 연구 요약 및 주요 결과
본 연구는 영화 포스터 이미지와 줄거리 텍스트를 통합적으로 활용하여 영화 장르를 예측하는 멀티모달 인공지능 모델을 제안하고, 다양한 융합 전략에 따른 성능을 비교·분석하였다. 이를 위해 공공 벤치마크 데이터셋인 MM‑IMDb를 활용하여 단일모달 및 멀티모달 모델을 모두 구현하였으며, 실험은 각각 동일한 학습 환경에서 수행되었다.
연구의 주요 목적은 다음 세 가지였다.
        1. 텍스트 단독 모델과 이미지 단독 모델의 성능을 비교하여 각 모달리티의 장단점을 규명한다.
        2. Early Fusion, Late Fusion, Attention Fusion, GMU, Cross‑Attention Fusion 등 다양한 멀티모달 융합 전략을 동일한 조건에서 비교·평가한다.
        3. Cross‑Attention 기반의 심층적 상호작용 구조가 영화 장르 예측의 일반화 성능에 미치는 영향을 검증한다.
연구 결과, 텍스트 기반 단일모달 모델 중에서는 BERT가 가장 우수한 성능을 보였다. 이는 영화의 서사 구조가 장르를 결정하는 핵심적인 단서로 작용함을 의미한다. 반면, 이미지 단일모달 모델에서는 ViT가 ResNet50 대비 상대적으로 우수한 결과를 기록하였다. ViT의 전역적 관계 학습 구조가 포스터의 색채 구성과 분위기 정보를 효과적으로 포착한 것으로 해석된다.
멀티모달 융합 모델은 모든 경우에서 단일모달 대비 성능이 향상되었으며, 다양한 융합 전략 중 Cross-Attention Fusion이 가장 높은 성능을 기록하였다. 이 모델은 텍스트와 이미지 간의 의미적 정렬을 구조적으로 달성함으로써, 두 모달리티의 상호 보완적 정보를 효과적으로 통합하였다. 다른 융합 방식들은 각각의 구조적 특성에 따라 성능 차이를 보였으며, 단순 병합 방식보다 모달 간 상호작용을 학습하는 구조가 더 효과적임을 보여주었다.
이상의 결과는 멀티모달 융합 전략이 단일모달 접근의 한계를 극복할 수 있으며, 융합 구조의 설계 방식이 예측 성능에 결정적인 영향을 미친다는 점을 실증적으로 보여준다.

    5.2. 시사점
본 연구는 멀티모달 학습의 구조적 설계가 예측 성능에 미치는 영향을 실험적으로 검증하였으며, 다음과 같은 학문적 및 실무적 시사점을 제공한다.
첫째, 텍스트 모달리티는 영화 장르 예측에서 가장 핵심적인 정보원으로 작용하며, 단일모달 환경에서도 높은 성능을 발휘할 수 있다. 이는 텍스트 기반 모델의 중요성과 함께, 텍스트 전처리 및 표현 방식의 정교함이 예측 성능에 직접적인 영향을 미친다는 점을 시사한다.
둘째, 멀티모달 융합은 단일모달 대비 평균적으로 30~120% 이상의 성능 향상을 가져오며, 이는 모달 간 정보 보완 효과를 실증적으로 입증한다. 특히 멀티레이블 분류와 같이 클래스 간 불균형이 존재하는 문제에서, 다양한 모달리티의 정보를 통합함으로써 예측의 균형성과 안정성을 확보할 수 있다.
셋째, 다양한 융합 전략은 구조적 설계에 따라 성능 차이를 보이며, 단순 병합 방식보다 모달 간 관계를 학습하는 구조가 예측 성능 향상에 더 효과적이다. 특히 Cross-Attention Fusion은 모달 간 의미적 정렬을 구조적으로 달성함으로써, 예측 안정성과 일반화 능력을 동시에 확보하였다.
마지막으로, 본 연구는 멀티모달 학습이 단순히 성능 향상에 그치지 않고, 정보 해석의 다양성과 표현력의 확장을 가능하게 한다는 점에서, 향후 다양한 분야에서의 적용 가능성과 학문적 확장성을 동시에 제시한다.

    5.3. 연구 한계 및 향후 연구 방향
본 연구는 의미 있는 결과를 도출했으나, 다음과 같은 한계가 존재한다.
        1. 데이터셋의 제약: MM‑IMDb는 영어권 중심의 데이터로 구성되어 있어 언어적 다양성이 부족하다. 다국어 영화 데이터셋을 활용할 경우 문화권별 장르 표현 차이를 반영한 보다 일반화된 모델 검증이 필요하다.
        2. 시각 모달의 단편적 표현: 포스터 이미지는 영화의 홍보용 정적 시각물로, 실제 영상 속 장면이나 감정선의 연속성을 모두 반영하지 못한다. 향후에는 예고편(video), 오디오(soundtrack) 등 다중 시각, 청각 모달을 포함한 확장형 멀티모달 모델이 연구되어야 할 것이다.
        3. 모델 복잡도와 연산 비용: Cross‑Attention 구조는 예측 성능을 크게 향상시키지만, 연산량 증가와 메모리 사용량 확대를 초래한다. 실제 플랫폼 적용을 위해서는 모델 경량화(pruning, knowledge distillation) 및 하드웨어 최적화 연구가 병행되어야 한다.

    5.4. 종합 결론
본 연구는 멀티모달 융합 전략이 영화 장르 예측의 정확도와 일반화 성능을 향상시킬 수 있음을 실증적으로 입증하였다. 특히 Cross‑Attention 기반 구조는 텍스트와 이미지 간 고차원적 상호작용을 효과적으로 학습하여, 단일모달 접근의 한계를 극복하였다. 이러한 결과는 멀티모달 학습이 향후 콘텐츠 분석, 영상 추천, 감정 인식 등 복합 데이터가 활용되는 다양한 분야에서 핵심적 접근법으로 자리 잡을 수 있음을 보여준다. 나아가 본 연구는 단순히 성능 비교에 그치지 않고, 모달 간 관계를 학습하는 아키텍처 설계의 중요성을 강조하였다. 이는 '무엇을 결합할 것인가'보다 '어떻게 결합할 것인가'가 성능의 본질적 결정 요인임을 실증하였다.
따라서 본 연구의 시사점은 다음과 같이 정리할 수 있다.
    1. 텍스트·이미지 융합 모델의 구조 설계가 성능 향상에 핵심적이다.
    2. Cross‑Attention 기법은 타 융합 방법 대비 높은 정확도와 F1‑Score를 달성하였다.
    3. 데이터 불균형 문제 해결과 설명가능성 향상은 향후 연구의 필수 과제이다.
    4. 멀티모달 연구는 학문적 차원을 넘어 AI 기반 영화 산업 전반의 효율화 및 지능화에 실질적으로 기여할 수 있다.
결론적으로, 본 연구는 멀티모달 융합 학습의 실제 효과를 실험적으로 검증함으로써 향후 시각·언어 복합 데이터를 다루는 인공지능 연구의 발전에 기초를 제공하였다.

참고문헌

[1] Arevalo, J. et al. (2017). Gated Multimodal Units for Information Fusion (GMU). arXiv. https://arxiv.org/abs/1702.01992 
[2] Radford, A. et al. (2021). Learning Transferable Visual Models from Natural Language Supervision (CLIP). ICML. https://arxiv.org/abs/2103.00020
[3] Jia, C. et al. (2021). Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision (ALIGN). ICML. https://arxiv.org/abs/2102.05918
[4] Kiela, D. et al. (2019). Supervised Multimodal Bitransformers for Classifying Images and Text. arXiv. https://arxiv.org/abs/1909.02950
[5] Li, X. et al. (2020). Hierarchical multimodal fusion for movie genre classification. Information Fusion.
[6] Li, J. et al. (2022). BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. ICML. https://arxiv.org/abs/2201.12086
[7] Liu, H. et al. (2023). Visual Instruction Tuning for LLMs (LLaVA). https://arxiv.org/abs/2304.08485
[8] He, K. et al. (2016). Deep Residual Learning for Image Recognition (ResNet). CVPR. https://arxiv.org/abs/1512.03385
[9] Dosovitskiy, A. et al. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT). ICLR. https://arxiv.org/abs/2010.11929
[10] Devlin, J. et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL. https://arxiv.org/abs/1810.04805
[11] Liu, Y. et al. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv. https://arxiv.org/abs/1907.11692
[12] Redmon, J. et al. (2016). You Only Look Once: Unified, Real-Time Object Detection. CVPR. https://arxiv.org/abs/1506.02640
[13] Loshchilov & Hutter, 2019. Decoupled Weight Decay Regularization, ICLR. https://arxiv.org/abs/1711.05101 
