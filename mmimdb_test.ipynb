{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abbf95b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oksk1\\workspace\\mmimdb_test\\.venv\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel, RobertaTokenizer, RobertaModel\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtimm\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "# MM-IMDb 멀티모달 영화 장르 예측 모델 비교 실험\n",
    "# Multi-Modal Movie Genre Prediction Model Comparison on MM-IMDb Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b19aeb",
   "metadata": {},
   "source": [
    "# 📑 MM-IMDb 멀티모달 영화 장르 예측 모델 개발 및 비교 실험\n",
    "\n",
    "## 🎯 연구 목표\n",
    "- **주제**: 멀티모달 융합 기반 영화 장르 예측 모델 개발\n",
    "- **데이터셋**: MM-IMDb (25,000편 영화, 포스터 이미지 + 줄거리 텍스트 + 23개 멀티라벨 장르)\n",
    "- **핵심 기여**: Cross-Attention 기반 융합 모델 제안으로 이미지·텍스트 간 상호작용 정교화\n",
    "\n",
    "## 🔬 실험 구성\n",
    "### 비교 모델군\n",
    "1. **텍스트 단일 모달**: BERT, RoBERTa\n",
    "2. **이미지 단일 모달**: ResNet50, Vision Transformer (ViT)\n",
    "3. **객체 탐지 기반**: YOLO, Faster R-CNN\n",
    "4. **멀티모달 융합**: Early Fusion, Late Fusion, Attention Fusion\n",
    "5. **제안 모델**: Cross-Attention Fusion\n",
    "\n",
    "### 평가 지표\n",
    "- Accuracy, Precision, Recall, F1-score, ROC-AUC, mAP\n",
    "\n",
    "### 설명가능성 (XAI)\n",
    "- Grad-CAM (이미지), Attention Map (텍스트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63654d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터셋 설정 및 구성\n",
    "# MM-IMDb Dataset Configuration\n",
    "\n",
    "# 데이터셋 경로 설정 (실제 경로에 맞게 수정 필요)\n",
    "DATASET_PATH = \"data/mmimdb\"  # MM-IMDb 데이터셋 경로\n",
    "IMAGE_PATH = os.path.join(DATASET_PATH, \"images\")\n",
    "METADATA_PATH = os.path.join(DATASET_PATH, \"dataset.json\")\n",
    "\n",
    "# 모델 설정\n",
    "IMAGE_SIZE = 224\n",
    "MAX_TEXT_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_GENRES = 23  # MM-IMDb 장르 수\n",
    "\n",
    "# 장르 라벨 정의 (MM-IMDb 23개 장르)\n",
    "GENRE_LABELS = [\n",
    "    'Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary',\n",
    "    'Drama', 'Family', 'Fantasy', 'History', 'Horror', 'Music', 'Musical', 'Mystery',\n",
    "    'News', 'Romance', 'Sci-Fi', 'Short', 'Sport', 'Thriller', 'War', 'Western'\n",
    "]\n",
    "\n",
    "print(f\"Dataset configuration:\")\n",
    "print(f\"- Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"- Max text length: {MAX_TEXT_LENGTH}\")\n",
    "print(f\"- Batch size: {BATCH_SIZE}\")\n",
    "print(f\"- Number of genres: {NUM_GENRES}\")\n",
    "print(f\"- Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"- Number of epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ae857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터 전처리 설정\n",
    "# Data Preprocessing Setup\n",
    "\n",
    "# 이미지 전처리 변환 (데이터 증강 포함)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 텍스트 전처리 토크나이저 초기화\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "print(\"데이터 전처리 설정 완료:\")\n",
    "print(f\"- 이미지 변환: 크기 조정, 정규화, 데이터 증강\")\n",
    "print(f\"- 텍스트 토크나이저: BERT, RoBERTa\")\n",
    "print(f\"- 최대 텍스트 길이: {MAX_TEXT_LENGTH} 토큰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MM-IMDb 데이터셋 클래스 정의\n",
    "# Custom Dataset Class for MM-IMDb\n",
    "\n",
    "class MMIMDbDataset(Dataset):\n",
    "    def __init__(self, metadata_file, image_dir, tokenizer, transform=None, max_length=512):\n",
    "        \"\"\"\n",
    "        MM-IMDb 데이터셋 클래스\n",
    "        Args:\n",
    "            metadata_file: 메타데이터 JSON 파일 경로\n",
    "            image_dir: 이미지 디렉토리 경로\n",
    "            tokenizer: 텍스트 토크나이저\n",
    "            transform: 이미지 변환\n",
    "            max_length: 텍스트 최대 길이\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # 메타데이터 로드\n",
    "        with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        self.genre_to_idx = {genre: idx for idx, genre in enumerate(GENRE_LABELS)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # 이미지 로드\n",
    "        image_path = os.path.join(self.image_dir, item['image'])\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except:\n",
    "            # 이미지 로드 실패시 빈 이미지 생성\n",
    "            image = torch.zeros(3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        \n",
    "        # 텍스트 토크나이징\n",
    "        plot = item.get('plot', '')\n",
    "        if isinstance(plot, list):\n",
    "            plot = ' '.join(plot)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            plot,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # 장르 라벨 (멀티라벨)\n",
    "        genres = item.get('genres', [])\n",
    "        label = torch.zeros(NUM_GENRES)\n",
    "        for genre in genres:\n",
    "            if genre in self.genre_to_idx:\n",
    "                label[self.genre_to_idx[genre]] = 1.0\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': label,\n",
    "            'text': plot\n",
    "        }\n",
    "\n",
    "print(\"MM-IMDb 데이터셋 클래스 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbac778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 단일 모달 모델 정의\n",
    "# Single Modality Models\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    \"\"\"BERT 기반 텍스트 분류 모델\"\"\"\n",
    "    def __init__(self, num_classes=NUM_GENRES, dropout=0.3):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.classifier(output)\n",
    "\n",
    "class RoBERTaClassifier(nn.Module):\n",
    "    \"\"\"RoBERTa 기반 텍스트 분류 모델\"\"\"\n",
    "    def __init__(self, num_classes=NUM_GENRES, dropout=0.3):\n",
    "        super(RoBERTaClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.classifier(output)\n",
    "\n",
    "class ResNetClassifier(nn.Module):\n",
    "    \"\"\"ResNet50 기반 이미지 분류 모델\"\"\"\n",
    "    def __init__(self, num_classes=NUM_GENRES, dropout=0.3):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        self.resnet = timm.create_model('resnet50', pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()  # 마지막 층 제거\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        output = self.dropout(features)\n",
    "        return self.classifier(output)\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    \"\"\"Vision Transformer 기반 이미지 분류 모델\"\"\"\n",
    "    def __init__(self, num_classes=NUM_GENRES, dropout=0.3):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "        self.vit.head = nn.Identity()  # 마지막 층 제거\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.vit(images)\n",
    "        output = self.dropout(features)\n",
    "        return self.classifier(output)\n",
    "\n",
    "print(\"단일 모달 모델 정의 완료:\")\n",
    "print(\"- BERT 텍스트 분류기\")\n",
    "print(\"- RoBERTa 텍스트 분류기\") \n",
    "print(\"- ResNet50 이미지 분류기\")\n",
    "print(\"- Vision Transformer 이미지 분류기\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092cc780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 멀티모달 융합 모델 정의\n",
    "# Multimodal Fusion Models\n",
    "\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    \"\"\"Early Fusion: 특징을 초기에 결합\"\"\"\n",
    "    def __init__(self, num_classes=NUM_GENRES, dropout=0.3):\n",
    "        super(EarlyFusionModel, self).__init__()\n",
    "        # 텍스트 인코더\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # 이미지 인코더\n",
    "        self.resnet = timm.create_model('resnet50', pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        # 융합 층\n",
    "        self.fusion_dim = self.bert.config.hidden_size + 2048\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.fusion_dim, num_classes)\n",
    "        \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # 텍스트 특징 추출\n",
    "        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.pooler_output\n",
    "        \n",
    "        # 이미지 특징 추출\n",
    "        image_features = self.resnet(images)\n",
    "        \n",
    "        # 특징 결합\n",
    "        fused_features = torch.cat([text_features, image_features], dim=1)\n",
    "        output = self.dropout(fused_features)\n",
    "        return self.classifier(output)\n",
    "\n",
    "class LateFusionModel(nn.Module):\n",
    "    \"\"\"Late Fusion: 각 모달리티를 독립적으로 학습 후 결과 결합\"\"\"\n",
    "    def __init__(self, num_classes=NUM_GENRES, dropout=0.3):\n",
    "        super(LateFusionModel, self).__init__()\n",
    "        # 텍스트 분류기\n",
    "        self.text_classifier = BERTClassifier(num_classes, dropout)\n",
    "        # 이미지 분류기\n",
    "        self.image_classifier = ResNetClassifier(num_classes, dropout)\n",
    "        \n",
    "        # 융합 가중치\n",
    "        self.fusion_weights = nn.Parameter(torch.tensor([0.5, 0.5]))\n",
    "        \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # 각 모달리티별 예측\n",
    "        text_logits = self.text_classifier(input_ids, attention_mask)\n",
    "        image_logits = self.image_classifier(images)\n",
    "        \n",
    "        # 가중 평균\n",
    "        weights = torch.softmax(self.fusion_weights, dim=0)\n",
    "        fused_logits = weights[0] * text_logits + weights[1] * image_logits\n",
    "        return fused_logits\n",
    "\n",
    "class AttentionFusionModel(nn.Module):\n",
    "    \"\"\"Attention Fusion: 어텐션 메커니즘으로 모달리티 중요도 결정\"\"\"\n",
    "    def __init__(self, num_classes=NUM_GENRES, dropout=0.3):\n",
    "        super(AttentionFusionModel, self).__init__()\n",
    "        # 특징 추출기\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.resnet = timm.create_model('resnet50', pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        # 어텐션 메커니즘\n",
    "        self.text_attention = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "        self.image_attention = nn.Linear(2048, 1)\n",
    "        \n",
    "        # 분류기\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + 2048, num_classes)\n",
    "        \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # 특징 추출\n",
    "        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.pooler_output\n",
    "        image_features = self.resnet(images)\n",
    "        \n",
    "        # 어텐션 가중치 계산\n",
    "        text_att = torch.sigmoid(self.text_attention(text_features))\n",
    "        image_att = torch.sigmoid(self.image_attention(image_features))\n",
    "        \n",
    "        # 어텐션 적용\n",
    "        weighted_text = text_att * text_features\n",
    "        weighted_image = image_att * image_features\n",
    "        \n",
    "        # 특징 결합\n",
    "        fused_features = torch.cat([weighted_text, weighted_image], dim=1)\n",
    "        output = self.dropout(fused_features)\n",
    "        return self.classifier(output)\n",
    "\n",
    "class CrossAttentionFusionModel(nn.Module):\n",
    "    \"\"\"Cross-Attention Fusion: 제안하는 모델\"\"\"\n",
    "    def __init__(self, num_classes=NUM_GENRES, dropout=0.3, d_model=512):\n",
    "        super(CrossAttentionFusionModel, self).__init__()\n",
    "        # 특징 추출기\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.resnet = timm.create_model('resnet50', pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        # 차원 정렬\n",
    "        self.text_proj = nn.Linear(self.bert.config.hidden_size, d_model)\n",
    "        self.image_proj = nn.Linear(2048, d_model)\n",
    "        \n",
    "        # Cross-Attention 층\n",
    "        self.cross_attention = nn.MultiheadAttention(d_model, num_heads=8, dropout=dropout)\n",
    "        \n",
    "        # 분류기\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(d_model * 2, num_classes)\n",
    "        \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # 특징 추출\n",
    "        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.pooler_output\n",
    "        image_features = self.resnet(images)\n",
    "        \n",
    "        # 차원 정렬\n",
    "        text_proj = self.text_proj(text_features).unsqueeze(0)  # [1, batch, d_model]\n",
    "        image_proj = self.image_proj(image_features).unsqueeze(0)  # [1, batch, d_model]\n",
    "        \n",
    "        # Cross-Attention\n",
    "        text_attended, _ = self.cross_attention(text_proj, image_proj, image_proj)\n",
    "        image_attended, _ = self.cross_attention(image_proj, text_proj, text_proj)\n",
    "        \n",
    "        # 특징 결합\n",
    "        fused_features = torch.cat([\n",
    "            text_attended.squeeze(0), \n",
    "            image_attended.squeeze(0)\n",
    "        ], dim=1)\n",
    "        \n",
    "        output = self.dropout(fused_features)\n",
    "        return self.classifier(output)\n",
    "\n",
    "print(\"멀티모달 융합 모델 정의 완료:\")\n",
    "print(\"- Early Fusion Model\")\n",
    "print(\"- Late Fusion Model\")\n",
    "print(\"- Attention Fusion Model\")\n",
    "print(\"- Cross-Attention Fusion Model (제안 모델)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75245d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 학습 및 평가 함수 정의\n",
    "# Training and Evaluation Functions\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE):\n",
    "    \"\"\"모델 학습 함수\"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # 손실 함수 및 옵티마이저\n",
    "    criterion = nn.BCEWithLogitsLoss()  # 멀티라벨 분류\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # 학습률 스케줄러\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 학습 단계\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training')\n",
    "        \n",
    "        for batch in train_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # 모델 유형에 따른 순전파\n",
    "            if isinstance(model, (BERTClassifier, RoBERTaClassifier)):\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            elif isinstance(model, (ResNetClassifier, ViTClassifier)):\n",
    "                outputs = model(images)\n",
    "            else:  # 멀티모달 모델\n",
    "                outputs = model(images, input_ids, attention_mask)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'Loss': loss.item()})\n",
    "        \n",
    "        # 검증 단계\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                if isinstance(model, (BERTClassifier, RoBERTaClassifier)):\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                elif isinstance(model, (ResNetClassifier, ViTClassifier)):\n",
    "                    outputs = model(images)\n",
    "                else:\n",
    "                    outputs = model(images, input_ids, attention_mask)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}')\n",
    "        \n",
    "        # 학습률 조정\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # 최적 모델 저장\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), f'best_model_{type(model).__name__}.pth')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, test_loader, model_name=\"Model\"):\n",
    "    \"\"\"모델 평가 함수\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f'Evaluating {model_name}'):\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            if isinstance(model, (BERTClassifier, RoBERTaClassifier)):\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "            elif isinstance(model, (ResNetClassifier, ViTClassifier)):\n",
    "                outputs = model(images)\n",
    "            else:\n",
    "                outputs = model(images, input_ids, attention_mask)\n",
    "            \n",
    "            # 시그모이드 활성화로 확률 계산\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "            \n",
    "            all_predictions.append(predictions.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probabilities.append(probabilities.cpu())\n",
    "    \n",
    "    # 결과 결합\n",
    "    all_predictions = torch.cat(all_predictions, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    all_probabilities = torch.cat(all_probabilities, dim=0).numpy()\n",
    "    \n",
    "    # 평가 지표 계산\n",
    "    metrics = calculate_metrics(all_labels, all_predictions, all_probabilities)\n",
    "    return metrics\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_prob):\n",
    "    \"\"\"평가 지표 계산 함수\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # 전체 정확도 (Exact Match)\n",
    "    exact_match = np.mean(np.all(y_true == y_pred, axis=1))\n",
    "    metrics['Exact_Match_Accuracy'] = exact_match\n",
    "    \n",
    "    # 라벨별 정확도\n",
    "    label_accuracy = np.mean(y_true == y_pred)\n",
    "    metrics['Label_Accuracy'] = label_accuracy\n",
    "    \n",
    "    # Precision, Recall, F1-score (Macro)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics['Macro_Precision'] = precision\n",
    "    metrics['Macro_Recall'] = recall\n",
    "    metrics['Macro_F1'] = f1\n",
    "    \n",
    "    # Precision, Recall, F1-score (Micro)\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(y_true, y_pred, average='micro', zero_division=0)\n",
    "    metrics['Micro_Precision'] = precision_micro\n",
    "    metrics['Micro_Recall'] = recall_micro\n",
    "    metrics['Micro_F1'] = f1_micro\n",
    "    \n",
    "    # ROC-AUC (Macro)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, y_prob, average='macro')\n",
    "        metrics['ROC_AUC_Macro'] = roc_auc\n",
    "    except ValueError:\n",
    "        metrics['ROC_AUC_Macro'] = 0.0\n",
    "    \n",
    "    # mAP (Mean Average Precision)\n",
    "    try:\n",
    "        map_score = average_precision_score(y_true, y_prob, average='macro')\n",
    "        metrics['mAP'] = map_score\n",
    "    except ValueError:\n",
    "        metrics['mAP'] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"학습 및 평가 함수 정의 완료:\")\n",
    "print(\"- train_model(): 모델 학습\")\n",
    "print(\"- evaluate_model(): 모델 평가\")\n",
    "print(\"- calculate_metrics(): 평가 지표 계산\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72451931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 데이터 로딩 및 분할\n",
    "# Data Loading and Splitting\n",
    "\n",
    "def load_and_split_data():\n",
    "    \"\"\"\n",
    "    MM-IMDb 데이터셋을 로드하고 훈련/검증/테스트로 분할\n",
    "    논문 계획에 따라 70% / 15% / 15%로 분할\n",
    "    \"\"\"\n",
    "    # 실제 데이터 로딩 (예시 코드 - 실제 경로에 맞게 수정 필요)\n",
    "    try:\n",
    "        # 메타데이터 로드\n",
    "        with open(METADATA_PATH, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"전체 데이터 개수: {len(data)}\")\n",
    "        \n",
    "        # 데이터 섞기\n",
    "        np.random.seed(42)\n",
    "        indices = np.random.permutation(len(data))\n",
    "        \n",
    "        # 분할 지점 계산\n",
    "        train_end = int(0.7 * len(data))\n",
    "        val_end = int(0.85 * len(data))\n",
    "        \n",
    "        train_indices = indices[:train_end]\n",
    "        val_indices = indices[train_end:val_end]\n",
    "        test_indices = indices[val_end:]\n",
    "        \n",
    "        # 분할된 데이터 생성\n",
    "        train_data = [data[i] for i in train_indices]\n",
    "        val_data = [data[i] for i in val_indices]\n",
    "        test_data = [data[i] for i in test_indices]\n",
    "        \n",
    "        print(f\"훈련 데이터: {len(train_data)} ({len(train_data)/len(data)*100:.1f}%)\")\n",
    "        print(f\"검증 데이터: {len(val_data)} ({len(val_data)/len(data)*100:.1f}%)\")\n",
    "        print(f\"테스트 데이터: {len(test_data)} ({len(test_data)/len(data)*100:.1f}%)\")\n",
    "        \n",
    "        return train_data, val_data, test_data\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"데이터셋 파일을 찾을 수 없습니다: {METADATA_PATH}\")\n",
    "        print(\"샘플 데이터를 생성합니다...\")\n",
    "        return create_sample_data()\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"\n",
    "    테스트용 샘플 데이터 생성 (실제 데이터가 없을 경우)\n",
    "    \"\"\"\n",
    "    sample_data = []\n",
    "    for i in range(1000):  # 1000개 샘플\n",
    "        sample_data.append({\n",
    "            'imdb_id': f'tt{i:07d}',\n",
    "            'image': f'movie_{i}.jpg',\n",
    "            'plot': f'This is a sample movie plot for movie {i}. It contains various elements of storytelling.',\n",
    "            'genres': np.random.choice(GENRE_LABELS, size=np.random.randint(1, 4), replace=False).tolist()\n",
    "        })\n",
    "    \n",
    "    # 70% / 15% / 15% 분할\n",
    "    train_data = sample_data[:700]\n",
    "    val_data = sample_data[700:850]\n",
    "    test_data = sample_data[850:]\n",
    "    \n",
    "    print(\"샘플 데이터 생성 완료:\")\n",
    "    print(f\"훈련 데이터: {len(train_data)}\")\n",
    "    print(f\"검증 데이터: {len(val_data)}\")\n",
    "    print(f\"테스트 데이터: {len(test_data)}\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def create_data_loaders(train_data, val_data, test_data, tokenizer):\n",
    "    \"\"\"데이터 로더 생성\"\"\"\n",
    "    # 데이터셋 생성\n",
    "    train_dataset = MMIMDbDataset(train_data, IMAGE_PATH, tokenizer, train_transform, MAX_TEXT_LENGTH)\n",
    "    val_dataset = MMIMDbDataset(val_data, IMAGE_PATH, tokenizer, val_transform, MAX_TEXT_LENGTH)\n",
    "    test_dataset = MMIMDbDataset(test_data, IMAGE_PATH, tokenizer, val_transform, MAX_TEXT_LENGTH)\n",
    "    \n",
    "    # 데이터 로더 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# 데이터 로드 및 분할 실행\n",
    "print(\"데이터 로딩 중...\")\n",
    "train_data, val_data, test_data = load_and_split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c62084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MM-IMDb 데이터셋 다운로드\n",
    "# MM-IMDb Dataset Download\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def download_mmimdb_dataset():\n",
    "    \"\"\"MM-IMDb 데이터셋 다운로드 및 압축 해제\"\"\"\n",
    "    \n",
    "    # 데이터셋 URL들\n",
    "    urls = {\n",
    "        'metadata': 'https://archive.org/download/mmimdb/mmimdb.tar.gz',\n",
    "        'images': 'https://archive.org/download/mmimdb/mmimdb.tar.gz'  # 같은 파일에 포함\n",
    "    }\n",
    "    \n",
    "    # 다운로드 디렉토리 생성\n",
    "    download_dir = Path(\"downloads\")\n",
    "    data_dir = Path(\"data/mmimdb\")\n",
    "    \n",
    "    download_dir.mkdir(exist_ok=True)\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"📥 MM-IMDb 데이터셋 다운로드를 시작합니다...\")\n",
    "    \n",
    "    # 데이터셋 다운로드\n",
    "    dataset_file = download_dir / \"mmimdb.tar.gz\"\n",
    "    \n",
    "    if not dataset_file.exists():\n",
    "        print(\"🌐 데이터셋 파일 다운로드 중...\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(urls['metadata'], stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            downloaded_size = 0\n",
    "            \n",
    "            with open(dataset_file, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        downloaded_size += len(chunk)\n",
    "                        \n",
    "                        # 진행률 표시\n",
    "                        if total_size > 0:\n",
    "                            percent = (downloaded_size / total_size) * 100\n",
    "                            print(f\"\\r진행률: {percent:.1f}% ({downloaded_size/1024/1024:.1f}MB / {total_size/1024/1024:.1f}MB)\", end='')\n",
    "            \n",
    "            print(f\"\\n✅ 다운로드 완료: {dataset_file}\")\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ 다운로드 실패: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"✅ 이미 다운로드됨: {dataset_file}\")\n",
    "    \n",
    "    # 압축 해제\n",
    "    print(\"📦 압축 파일 해제 중...\")\n",
    "    try:\n",
    "        with tarfile.open(dataset_file, 'r:gz') as tar:\n",
    "            # 압축 파일 내용 확인\n",
    "            members = tar.getmembers()\n",
    "            print(f\"압축 파일 내 파일 수: {len(members)}\")\n",
    "            \n",
    "            # 진행률과 함께 압축 해제\n",
    "            for i, member in enumerate(members):\n",
    "                tar.extract(member, path=download_dir)\n",
    "                if i % 100 == 0:  # 100개마다 진행률 업데이트\n",
    "                    percent = (i / len(members)) * 100\n",
    "                    print(f\"\\r압축 해제 진행률: {percent:.1f}%\", end='')\n",
    "            \n",
    "            print(f\"\\n✅ 압축 해제 완료\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 압축 해제 실패: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # 파일 정리\n",
    "    print(\"📁 파일 정리 중...\")\n",
    "    \n",
    "    # 압축 해제된 디렉토리 찾기\n",
    "    extracted_dirs = [d for d in download_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    if extracted_dirs:\n",
    "        source_dir = extracted_dirs[0]  # 첫 번째 디렉토리\n",
    "        \n",
    "        # 파일들을 data/mmimdb로 이동\n",
    "        for item in source_dir.rglob('*'):\n",
    "            if item.is_file():\n",
    "                # 상대 경로 계산\n",
    "                rel_path = item.relative_to(source_dir)\n",
    "                dest_path = data_dir / rel_path\n",
    "                \n",
    "                # 대상 디렉토리 생성\n",
    "                dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                # 파일 이동\n",
    "                shutil.copy2(item, dest_path)\n",
    "        \n",
    "        print(f\"✅ 파일 정리 완료: {data_dir}\")\n",
    "    \n",
    "    # 다운로드 임시 파일 정리\n",
    "    print(\"🧹 임시 파일 정리 중...\")\n",
    "    if dataset_file.exists():\n",
    "        dataset_file.unlink()\n",
    "    \n",
    "    for temp_dir in extracted_dirs:\n",
    "        if temp_dir.exists():\n",
    "            shutil.rmtree(temp_dir)\n",
    "    \n",
    "    # 결과 확인\n",
    "    print(\"\\n📊 다운로드 결과:\")\n",
    "    if data_dir.exists():\n",
    "        file_count = len(list(data_dir.rglob('*')))\n",
    "        print(f\"- 저장 위치: {data_dir}\")\n",
    "        print(f\"- 총 파일 수: {file_count}\")\n",
    "        \n",
    "        # 주요 파일들 확인\n",
    "        important_files = ['dataset.json', 'split.json']\n",
    "        for file_name in important_files:\n",
    "            file_path = data_dir / file_name\n",
    "            if file_path.exists():\n",
    "                print(f\"- ✅ {file_name} 발견\")\n",
    "            else:\n",
    "                print(f\"- ❌ {file_name} 없음\")\n",
    "        \n",
    "        # 이미지 디렉토리 확인\n",
    "        image_dirs = ['images', 'imgs', 'posters']\n",
    "        for dir_name in image_dirs:\n",
    "            dir_path = data_dir / dir_name\n",
    "            if dir_path.exists() and dir_path.is_dir():\n",
    "                image_count = len(list(dir_path.glob('*')))\n",
    "                print(f\"- ✅ {dir_name} 디렉토리: {image_count}개 파일\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def download_alternative_mmimdb():\n",
    "    \"\"\"대안 다운로드 방법 (Kaggle 등)\"\"\"\n",
    "    print(\"🔄 대안 다운로드 방법을 시도합니다...\")\n",
    "    \n",
    "    # GitHub에서 샘플 데이터 또는 다른 소스 시도\n",
    "    alternative_urls = [\n",
    "        \"https://github.com/johnarevalo/mmimdb/archive/refs/heads/master.zip\",\n",
    "        \"https://raw.githubusercontent.com/johnarevalo/mmimdb/master/dataset.json\"\n",
    "    ]\n",
    "    \n",
    "    data_dir = Path(\"data/mmimdb\")\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for i, url in enumerate(alternative_urls):\n",
    "        try:\n",
    "            print(f\"시도 {i+1}: {url}\")\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            if url.endswith('.zip'):\n",
    "                file_path = data_dir / \"mmimdb_alt.zip\"\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                \n",
    "                # ZIP 압축 해제\n",
    "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(data_dir)\n",
    "                file_path.unlink()  # ZIP 파일 삭제\n",
    "                \n",
    "            elif url.endswith('.json'):\n",
    "                file_path = data_dir / \"dataset.json\"\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            \n",
    "            print(f\"✅ 성공: {url}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 실패: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return False\n",
    "\n",
    "# 실행\n",
    "print(\"MM-IMDb 데이터셋 다운로드를 시작합니다...\\n\")\n",
    "\n",
    "success = download_mmimdb_dataset()\n",
    "\n",
    "if not success:\n",
    "    print(\"\\n기본 다운로드 실패. 대안 방법을 시도합니다...\")\n",
    "    success = download_alternative_mmimdb()\n",
    "\n",
    "if not success:\n",
    "    print(\"\\n❌ 모든 다운로드 방법이 실패했습니다.\")\n",
    "    print(\"수동으로 다음 작업을 수행해주세요:\")\n",
    "    print(\"1. https://archive.org/details/mmimdb 에서 데이터셋 다운로드\")\n",
    "    print(\"2. 압축 해제 후 data/mmimdb 폴더에 저장\")\n",
    "    print(\"3. 또는 Kaggle, GitHub 등에서 MM-IMDb 데이터셋 검색\")\n",
    "else:\n",
    "    print(f\"\\n🎉 MM-IMDb 데이터셋 다운로드 완료!\")\n",
    "    print(f\"저장 위치: data/mmimdb\")\n",
    "    print(\"이제 다음 셀에서 데이터를 로드할 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4686b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 모델 실험 실행\n",
    "# Model Experiments Execution\n",
    "\n",
    "# 실험할 모델들 정의\n",
    "models_to_test = {\n",
    "    'BERT': BERTClassifier(),\n",
    "    'RoBERTa': RoBERTaClassifier(),\n",
    "    'ResNet50': ResNetClassifier(),\n",
    "    'ViT': ViTClassifier(),\n",
    "    'Early_Fusion': EarlyFusionModel(),\n",
    "    'Late_Fusion': LateFusionModel(),\n",
    "    'Attention_Fusion': AttentionFusionModel(),\n",
    "    'Cross_Attention_Fusion': CrossAttentionFusionModel()  # 제안 모델\n",
    "}\n",
    "\n",
    "# 실험 결과 저장용 딕셔너리\n",
    "experiment_results = {}\n",
    "\n",
    "def run_experiment(model_name, model, train_loader, val_loader, test_loader):\n",
    "    \"\"\"단일 모델 실험 실행\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"실험 시작: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 모델 학습\n",
    "    print(\"모델 학습 중...\")\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader)\n",
    "    \n",
    "    # 최적 모델 로드\n",
    "    model.load_state_dict(torch.load(f'best_model_{type(model).__name__}.pth'))\n",
    "    \n",
    "    # 모델 평가\n",
    "    print(\"모델 평가 중...\")\n",
    "    metrics = evaluate_model(model, test_loader, model_name)\n",
    "    \n",
    "    # 결과 저장\n",
    "    experiment_results[model_name] = {\n",
    "        'metrics': metrics,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f\"\\n{model_name} 실험 결과:\")\n",
    "    print(\"-\" * 30)\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# 축약된 실험 (시간 절약을 위해)\n",
    "print(\"축약된 실험을 실행합니다 (각 모델 5 에포크)...\")\n",
    "NUM_EPOCHS = 5  # 빠른 테스트를 위해 에포크 수 줄임\n",
    "\n",
    "# BERT 텍스트 모델만 우선 테스트 (예시)\n",
    "print(\"BERT 모델 테스트를 위한 데이터 로더 생성...\")\n",
    "try:\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        train_data, val_data, test_data, bert_tokenizer\n",
    "    )\n",
    "    print(\"데이터 로더 생성 완료\")\n",
    "    \n",
    "    # BERT 모델 테스트\n",
    "    bert_model = BERTClassifier()\n",
    "    print(\"BERT 모델 초기화 완료\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"데이터 로더 생성 중 오류 발생: {e}\")\n",
    "    print(\"실제 데이터가 없어 모델 구조만 확인합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 결과 시각화 및 분석\n",
    "# Results Visualization and Analysis\n",
    "\n",
    "def plot_training_curves(experiment_results):\n",
    "    \"\"\"학습 곡선 시각화\"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (model_name, results) in enumerate(experiment_results.items()):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[idx]\n",
    "        train_losses = results['train_losses']\n",
    "        val_losses = results['val_losses']\n",
    "        \n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        ax.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "        ax.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "        ax.set_title(f'{model_name} Learning Curves')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # 빈 subplot 숨기기\n",
    "    for idx in range(len(experiment_results), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_performance_comparison(experiment_results):\n",
    "    \"\"\"모델 성능 비교 시각화\"\"\"\n",
    "    if not experiment_results:\n",
    "        print(\"실험 결과가 없습니다. 먼저 모델을 학습해주세요.\")\n",
    "        return\n",
    "    \n",
    "    # 주요 지표들 추출\n",
    "    metrics_to_plot = ['Exact_Match_Accuracy', 'Label_Accuracy', 'Macro_F1', 'ROC_AUC_Macro', 'mAP']\n",
    "    \n",
    "    model_names = list(experiment_results.keys())\n",
    "    metric_data = {metric: [] for metric in metrics_to_plot}\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        for metric in metrics_to_plot:\n",
    "            value = experiment_results[model_name]['metrics'].get(metric, 0)\n",
    "            metric_data[metric].append(value)\n",
    "    \n",
    "    # 히트맵 생성\n",
    "    df = pd.DataFrame(metric_data, index=model_names)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df, annot=True, cmap='YlOrRd', fmt='.3f', cbar=True)\n",
    "    plt.title('Model Performance Comparison Heatmap')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Models')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 막대 그래프\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[idx]\n",
    "        values = metric_data[metric]\n",
    "        bars = ax.bar(model_names, values, color='skyblue', alpha=0.7)\n",
    "        ax.set_title(f'{metric} Comparison')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 최고 성능 모델 강조\n",
    "        if values:\n",
    "            max_idx = values.index(max(values))\n",
    "            bars[max_idx].set_color('orange')\n",
    "        \n",
    "        # 값 표시\n",
    "        for bar, value in zip(bars, values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                   f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 빈 subplot 숨기기\n",
    "    for idx in range(len(metrics_to_plot), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_performance_table(experiment_results):\n",
    "    \"\"\"성능 비교 테이블 생성\"\"\"\n",
    "    if not experiment_results:\n",
    "        print(\"실험 결과가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    # 성능 테이블 생성\n",
    "    metrics_to_include = [\n",
    "        'Exact_Match_Accuracy', 'Label_Accuracy', 'Macro_Precision', \n",
    "        'Macro_Recall', 'Macro_F1', 'ROC_AUC_Macro', 'mAP'\n",
    "    ]\n",
    "    \n",
    "    table_data = []\n",
    "    for model_name, results in experiment_results.items():\n",
    "        row = [model_name]\n",
    "        for metric in metrics_to_include:\n",
    "            value = results['metrics'].get(metric, 0)\n",
    "            row.append(f\"{value:.4f}\")\n",
    "        table_data.append(row)\n",
    "    \n",
    "    # DataFrame으로 변환\n",
    "    columns = ['Model'] + metrics_to_include\n",
    "    df = pd.DataFrame(table_data, columns=columns)\n",
    "    \n",
    "    # 최고 성능 찾기\n",
    "    print(\"📊 모델 성능 비교 결과\")\n",
    "    print(\"=\" * 120)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    # 최고 성능 모델 요약\n",
    "    for metric in metrics_to_include[1:]:  # Model 컬럼 제외\n",
    "        values = [float(row[metrics_to_include.index(metric)]) for row in table_data]\n",
    "        best_idx = values.index(max(values))\n",
    "        best_model = table_data[best_idx][0]\n",
    "        best_value = max(values)\n",
    "        print(f\"🏆 {metric} 최고 성능: {best_model} ({best_value:.4f})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 샘플 결과 시각화 (실제 실험 후 사용)\n",
    "print(\"결과 시각화 함수 정의 완료:\")\n",
    "print(\"- plot_training_curves(): 학습 곡선 그래프\")\n",
    "print(\"- plot_performance_comparison(): 성능 비교 히트맵 및 막대그래프\")\n",
    "print(\"- generate_performance_table(): 성능 비교 테이블\")\n",
    "print(\"\\n실제 모델 학습 후 다음과 같이 사용하세요:\")\n",
    "print(\"plot_training_curves(experiment_results)\")\n",
    "print(\"plot_performance_comparison(experiment_results)\")\n",
    "print(\"generate_performance_table(experiment_results)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cfc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 설명가능성 분석 (XAI) - Grad-CAM & Attention Map\n",
    "# Explainable AI Analysis\n",
    "\n",
    "import cv2\n",
    "from matplotlib import cm\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"Grad-CAM 구현 클래스\"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Hook 등록\n",
    "        self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.target_layer.register_backward_hook(self.save_gradient)\n",
    "    \n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "    \n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "    \n",
    "    def generate_cam(self, input_image, class_idx=None):\n",
    "        \"\"\"Grad-CAM 생성\"\"\"\n",
    "        # 순전파\n",
    "        output = self.model(input_image)\n",
    "        \n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1)\n",
    "        \n",
    "        # 역전파\n",
    "        self.model.zero_grad()\n",
    "        class_score = output[:, class_idx].squeeze()\n",
    "        class_score.backward(retain_graph=True)\n",
    "        \n",
    "        # Grad-CAM 계산\n",
    "        gradients = self.gradients[0]  # [C, H, W]\n",
    "        activations = self.activations[0]  # [C, H, W]\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        weights = torch.mean(gradients, dim=(1, 2))  # [C]\n",
    "        \n",
    "        # 가중 합\n",
    "        cam = torch.zeros(activations.shape[1:])  # [H, W]\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i, :, :]\n",
    "        \n",
    "        # ReLU 적용\n",
    "        cam = torch.relu(cam)\n",
    "        \n",
    "        # 정규화\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / cam.max()\n",
    "        \n",
    "        return cam.cpu().numpy()\n",
    "\n",
    "def visualize_gradcam(model, image, genre_idx, image_transform):\n",
    "    \"\"\"Grad-CAM 시각화\"\"\"\n",
    "    # ResNet 기반 모델의 마지막 컨볼루션 레이어 찾기\n",
    "    if hasattr(model, 'resnet'):\n",
    "        target_layer = model.resnet.layer4[-1].conv3\n",
    "    elif hasattr(model, 'vit'):\n",
    "        # ViT의 경우 다른 방식 필요\n",
    "        print(\"ViT 모델의 Grad-CAM은 별도 구현이 필요합니다.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"이미지 특징 추출 레이어를 찾을 수 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    # Grad-CAM 생성\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    model.eval()\n",
    "    \n",
    "    # 입력 이미지 준비\n",
    "    input_tensor = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    # CAM 생성\n",
    "    cam = gradcam.generate_cam(input_tensor, genre_idx)\n",
    "    \n",
    "    # 원본 이미지로 변환 (정규화 해제)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    \n",
    "    original_image = image.clone()\n",
    "    for t, m, s in zip(original_image, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    original_image = torch.clamp(original_image, 0, 1)\n",
    "    \n",
    "    # 시각화\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # 원본 이미지\n",
    "    axes[0].imshow(original_image.permute(1, 2, 0))\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Grad-CAM 히트맵\n",
    "    axes[1].imshow(cam, cmap='jet')\n",
    "    axes[1].set_title(f'Grad-CAM for {GENRE_LABELS[genre_idx]}')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # 오버레이\n",
    "    overlay = original_image.permute(1, 2, 0).numpy()\n",
    "    cam_resized = cv2.resize(cam, (overlay.shape[1], overlay.shape[0]))\n",
    "    cam_colored = cm.jet(cam_resized)[:, :, :3]\n",
    "    \n",
    "    overlay_image = 0.6 * overlay + 0.4 * cam_colored\n",
    "    axes[2].imshow(overlay_image)\n",
    "    axes[2].set_title('Overlay')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class AttentionVisualizer:\n",
    "    \"\"\"어텐션 맵 시각화 클래스\"\"\"\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def extract_attention_weights(self, input_ids, attention_mask):\n",
    "        \"\"\"어텐션 가중치 추출\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # BERT의 경우\n",
    "        if hasattr(self.model, 'bert'):\n",
    "            outputs = self.model.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_attentions=True\n",
    "            )\n",
    "            attentions = outputs.attentions  # Tuple of attention weights\n",
    "            \n",
    "            # 마지막 레이어의 첫 번째 헤드 사용\n",
    "            attention_weights = attentions[-1][0, 0, :, :].detach().cpu().numpy()\n",
    "            return attention_weights\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def visualize_attention(self, text, input_ids, attention_mask):\n",
    "        \"\"\"어텐션 맵 시각화\"\"\"\n",
    "        attention_weights = self.extract_attention_weights(input_ids, attention_mask)\n",
    "        \n",
    "        if attention_weights is None:\n",
    "            print(\"어텐션 가중치를 추출할 수 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        # 토큰 디코딩\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        \n",
    "        # [CLS] 토큰의 어텐션만 사용 (첫 번째 토큰)\n",
    "        cls_attention = attention_weights[0, :]\n",
    "        \n",
    "        # 유효한 토큰만 선별 (패딩 제외)\n",
    "        valid_length = attention_mask.sum().item()\n",
    "        tokens = tokens[:valid_length]\n",
    "        cls_attention = cls_attention[:valid_length]\n",
    "        \n",
    "        # 시각화\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # 어텐션 히트맵\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.imshow(cls_attention.reshape(1, -1), cmap='Blues', aspect='auto')\n",
    "        plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "        plt.yticks([0], ['[CLS]'])\n",
    "        plt.title('Attention Weights from [CLS] Token')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # 어텐션 막대 그래프\n",
    "        plt.subplot(2, 1, 2)\n",
    "        bars = plt.bar(range(len(tokens)), cls_attention, color='skyblue', alpha=0.7)\n",
    "        plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "        plt.ylabel('Attention Weight')\n",
    "        plt.title('Token-wise Attention Weights')\n",
    "        \n",
    "        # 높은 어텐션 토큰 강조\n",
    "        top_indices = cls_attention.argsort()[-5:]  # 상위 5개\n",
    "        for idx in top_indices:\n",
    "            bars[idx].set_color('orange')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 중요한 토큰들 출력\n",
    "        print(\"🔍 높은 어텐션을 받은 토큰들:\")\n",
    "        for idx in top_indices[::-1]:\n",
    "            print(f\"  '{tokens[idx]}': {cls_attention[idx]:.4f}\")\n",
    "\n",
    "def run_xai_analysis(model, sample_batch, model_name=\"Model\"):\n",
    "    \"\"\"XAI 분석 실행\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"설명가능성 분석: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # 샘플 데이터 선택\n",
    "    sample_image = sample_batch['image'][0]\n",
    "    sample_input_ids = sample_batch['input_ids'][0:1]\n",
    "    sample_attention_mask = sample_batch['attention_mask'][0:1]\n",
    "    sample_text = sample_batch['text'][0]\n",
    "    sample_labels = sample_batch['labels'][0]\n",
    "    \n",
    "    # 예측 수행\n",
    "    with torch.no_grad():\n",
    "        if isinstance(model, (BERTClassifier, RoBERTaClassifier)):\n",
    "            outputs = model(sample_input_ids.to(device), sample_attention_mask.to(device))\n",
    "        elif isinstance(model, (ResNetClassifier, ViTClassifier)):\n",
    "            outputs = model(sample_image.unsqueeze(0).to(device))\n",
    "        else:  # 멀티모달 모델\n",
    "            outputs = model(\n",
    "                sample_image.unsqueeze(0).to(device),\n",
    "                sample_input_ids.to(device),\n",
    "                sample_attention_mask.to(device)\n",
    "            )\n",
    "    \n",
    "    probabilities = torch.sigmoid(outputs)\n",
    "    predictions = (probabilities > 0.5).float()\n",
    "    \n",
    "    # 예측된 장르들\n",
    "    predicted_genres = [GENRE_LABELS[i] for i, pred in enumerate(predictions[0]) if pred == 1]\n",
    "    actual_genres = [GENRE_LABELS[i] for i, label in enumerate(sample_labels) if label == 1]\n",
    "    \n",
    "    print(f\"실제 장르: {actual_genres}\")\n",
    "    print(f\"예측 장르: {predicted_genres}\")\n",
    "    print()\n",
    "    \n",
    "    # 이미지 모델인 경우 Grad-CAM\n",
    "    if isinstance(model, (ResNetClassifier, ViTClassifier)) or hasattr(model, 'resnet'):\n",
    "        print(\"Grad-CAM 분석 중...\")\n",
    "        if predicted_genres:\n",
    "            top_genre_idx = GENRE_LABELS.index(predicted_genres[0])\n",
    "            visualize_gradcam(model, sample_image, top_genre_idx, val_transform)\n",
    "    \n",
    "    # 텍스트 모델인 경우 Attention Map\n",
    "    if isinstance(model, (BERTClassifier, RoBERTaClassifier)) or hasattr(model, 'bert'):\n",
    "        print(\"Attention Map 분석 중...\")\n",
    "        if isinstance(model, BERTClassifier) or hasattr(model, 'bert'):\n",
    "            visualizer = AttentionVisualizer(model, bert_tokenizer)\n",
    "            visualizer.visualize_attention(sample_text, sample_input_ids, sample_attention_mask)\n",
    "\n",
    "print(\"설명가능성 분석 도구 정의 완료:\")\n",
    "print(\"- GradCAM: 이미지 영역 중요도 시각화\")\n",
    "print(\"- AttentionVisualizer: 텍스트 토큰 중요도 시각화\")\n",
    "print(\"- run_xai_analysis(): 통합 XAI 분석 실행\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad553492",
   "metadata": {},
   "source": [
    "# 📋 실험 결론 및 요약\n",
    "\n",
    "## 🎯 연구 목표 달성도\n",
    "\n",
    "### 1. 모델 비교 분석\n",
    "- **단일 모달리티 모델**: BERT, RoBERTa (텍스트), ResNet50, ViT (이미지)\n",
    "- **멀티모달 융합 모델**: Early Fusion, Late Fusion, Attention Fusion\n",
    "- **제안 모델**: Cross-Attention Fusion\n",
    "\n",
    "### 2. 평가 지표\n",
    "- **정확도 지표**: Exact Match Accuracy, Label Accuracy\n",
    "- **분류 성능**: Macro/Micro Precision, Recall, F1-score\n",
    "- **순위 기반**: ROC-AUC, mAP (Mean Average Precision)\n",
    "\n",
    "### 3. 설명가능성 (XAI)\n",
    "- **이미지 분석**: Grad-CAM을 통한 포스터 내 중요 영역 시각화\n",
    "- **텍스트 분석**: Attention Map을 통한 줄거리 내 중요 토큰 식별\n",
    "\n",
    "## 🔬 주요 발견사항\n",
    "\n",
    "### 예상 결과\n",
    "1. **Cross-Attention Fusion 모델**이 기존 융합 방식 대비 우수한 성능 예상\n",
    "2. **멀티모달 모델**이 단일 모달리티 모델보다 높은 성능 예상\n",
    "3. **텍스트 정보**가 이미지보다 장르 예측에 더 중요할 것으로 예상\n",
    "\n",
    "### 성능 비교 순서 (예상)\n",
    "1. Cross-Attention Fusion (제안 모델)\n",
    "2. Attention Fusion\n",
    "3. Late Fusion\n",
    "4. Early Fusion\n",
    "5. BERT/RoBERTa (텍스트 단일)\n",
    "6. ResNet50/ViT (이미지 단일)\n",
    "\n",
    "## 📊 활용 방법\n",
    "\n",
    "### 실험 실행 순서\n",
    "1. **환경 설정**: 필요한 라이브러리 설치 및 GPU 설정\n",
    "2. **데이터 준비**: MM-IMDb 데이터셋 다운로드 및 경로 설정\n",
    "3. **모델 학습**: 각 모델별 순차 학습 및 검증\n",
    "4. **성능 평가**: 테스트 데이터셋으로 최종 평가\n",
    "5. **결과 분석**: 시각화 및 XAI 분석\n",
    "\n",
    "### 실제 데이터 사용시 수정사항\n",
    "- `DATASET_PATH`, `IMAGE_PATH`, `METADATA_PATH` 변수를 실제 경로로 수정\n",
    "- 데이터셋 형식에 맞게 `MMIMDbDataset` 클래스 조정\n",
    "- GPU 메모리에 따라 `BATCH_SIZE` 조정\n",
    "- 수렴 속도에 따라 `NUM_EPOCHS` 조정\n",
    "\n",
    "## 🚀 확장 가능성\n",
    "\n",
    "### 추가 실험 아이디어\n",
    "1. **객체 탐지 통합**: YOLO, Faster R-CNN 특징 활용\n",
    "2. **데이터 증강**: 더 다양한 이미지/텍스트 증강 기법\n",
    "3. **앙상블 학습**: 여러 모델의 예측 결합\n",
    "4. **하이퍼파라미터 최적화**: Optuna 등을 활용한 자동 튜닝\n",
    "5. **전이 학습**: 다른 영화 데이터셋으로 일반화 성능 검증\n",
    "\n",
    "### 논문 기여점\n",
    "- Cross-Attention 기반 멀티모달 융합 방법론 제안\n",
    "- MM-IMDb 데이터셋에서의 체계적인 모델 비교 분석\n",
    "- 설명가능성 관점에서의 모델 해석 및 분석"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
